#!/usr/bin/env python3
import os, sys
import subprocess
import errno
import yaml
import argparse
import etcd3
import json
import threading, queue
import time
import re
import socket
import logging
import copy
from ldmsd.ldmsd_communicator import Communicator, fmt_status
from ldmsd.parser_util import *
from maestro.maestro_util import *

def agg_hosts(grp_name, aggs, daemons):
    return [ [ dmn, daemons[dmn]['endpoints'][ep]] for dmn in aggs[grp_name] for ep in daemons[dmn]['endpoints'] if 'maestro_comm' in daemons[dmn]['endpoints'][ep] ]

class MaestroMonitor(object):
    def __init__(self, spec, args):
        self.prefix = args.prefix
        self.lock = threading.Lock()
        if self.prefix[0] == '/':
            self.prefix = self.prefix[1:]
        self.spec = spec
        # Use the 1st etcd host for now
        etcd_host = spec['members'][0]['host']
        etcd_port = spec['members'][0]['port']
        self.client = etcd3.client(host=etcd_host, port=etcd_port,
                                   grpc_options=[ ('grpc.max_send_message_length',16*1024*1024),
                                                  ('grpc.max_receive_message_length',16*1024*1024)])
        self.syncobj_cond = None
        self.syncobj = None
        self.maestro_members = spec.get('maestro_members')
        if self.maestro_members:
            # maestros on RAFT
            from pysyncobj import SyncObj, SyncObjConf
            from pysyncobj.batteries import ReplLockManager, ReplDict
            from maestro.maestro_raft import DaemonDict
            self.syncobj_cond = threading.Condition(self.lock)
            myhost = socket.gethostname()
            # sort members by host
            self.maestro_members.sort(key = lambda m: m['host'])
            self.maestro_cluster = copy.deepcopy(self.maestro_members)
            # Check host uniqueness
            self.myaddr = None
            otheraddrs = list()
            prev_host = None
            for m in self.maestro_members:
                # sorted by host
                host = m['host']
                port = m['port']
                if host == prev_host:
                    raise RuntimeError(f'{host} appeared more than one time'
                                        ' in the maestro_members list')
                if host == myhost:
                    self.myaddr = f'{host}:{port}'
                else:
                    otheraddrs.append(f'{host}:{port}')
                prev_host = host
            if not self.myaddr:
                raise RuntimeError(f'`{myhost}` is not in the maestro_members list')
            syncobjconf = SyncObjConf()
            syncobjconf.onStateChanged = self.on_syncobj_state_changed
            self.dmn_dict = DaemonDict()
            self.ldmsd_mi_state = DaemonDict()
            self.do_raft_balance = ReplDict()
            self.raftLock = ReplLockManager(autoUnlockTime=30)
            self.syncobj = SyncObj(self.myaddr, otheraddrs, conf=syncobjconf, consumers=[self.dmn_dict,
                                                                                         self.raftLock,
                                                                                         self.do_raft_balance,
                                                                                         self.ldmsd_mi_state])
            while not self.raftLock.tryAcquire('balance_state', sync=True):
                self.raft_wait()
            for mi in self.maestro_members:
                self.do_raft_balance.set(f'{mi["host"]}:{mi["port"]}', 0, sync=True)
            self.raftLock.release('balance_state')
        self.args = args
        if self.args.rebalance != 'sets' and self.args.rebalance != 'producers':
            print(f'"rebalance" parameter must be assigned to either "sets" or "producers"\n'\
                   'If no argument is specified, maestro will balance across producers by default')
            sys.exit(1)
        self.do_rebalance = True
        self.res_samplers = False
        self.config = self.load_config()
        self.reset_conf()
        self.raft_handler(self.raft_balance)
        start_agg_daemons(self.aggregators, self.args.start_aggregators)
        self.comms = self.build_comms()
        self.raft_handler(self.init_mi_state)
        self.etcd_event_handler = {
            'endpoints'   : self.__reconf_endpoints,
            'daemons'     : self.__reconf_daemons,
            'aggregators' : self.__reconf_aggregators,
            'producers'   : self.__reconf_producers,
            'samplers'    : self.__reconf_samplers,
            'stores'      : self.__reconf_stores,
            'updaters'    : self.__reconf_updaters
        }
        self.rebalance_handler = {
            'producers' : self.__rebalance_prdcrs,
            'sets'      : self.__rebalance_sets
        }

    def on_syncobj_state_changed(self, old_state, new_state):
        self.lock.acquire()
        log.info(f"syncobj state changed {raft_state_str(old_state)} =>"
                 f" {raft_state_str(new_state)}")
        self.syncobj_cond.notify()
        self.lock.release()

    def raft_wait(self):
        time.sleep(3)
        while not self.syncobj.isReady():
            time.sleep(3)
        return

    def raft_balance(self):
        self.my_daemons = {}
        self.raft_wait()
        if self.do_raft_balance.get(self.myaddr) != 0:
            self.reset_conf()
        if self.syncobj._isLeader():
            self.my_daemons = copy.deepcopy(self.daemons)
            for mi in self.maestro_cluster:
                self.dmn_dict.addMaestroGroup(f'{mi["host"]}:{mi["port"]}', {}, sync=True)
            if self.syncobj.isReady():
                while not self.raftLock.tryAcquire('raft_balance', sync=True):
                    self.raft_wait()
                for dmn_grp in self.aggregators:
                    agg_dist = dist_list(expand_names(dmn_grp), len(self.maestro_cluster))
                    for p_, mi in zip(agg_dist, self.maestro_cluster):
                        for dmn in p_:
                            if dmn in self.my_daemons:
                                self.dmn_dict.setDaemon(f'{mi["host"]}:{mi["port"]}', dmn, self.my_daemons.pop(dmn), sync=True)
                for dmn_grp in self.samplers:
                    samp_dist = dist_list(expand_names(dmn_grp), len(self.maestro_cluster))
                    for p_, mi in zip(samp_dist, self.maestro_cluster):
                        for dmn in p_:
                            if dmn in self.my_daemons:
                                self.dmn_dict.setDaemon(f'{mi["host"]}:{mi["port"]}', dmn, self.my_daemons.pop(dmn), sync=True)
                self.my_daemons = self.dmn_dict[self.myaddr]
                self.raftLock.release('raft_balance')
        else:
            x = True
            while x==True:
                if self.raftLock.tryAcquire('raft_balance', sync=True):
                    self.my_daemons = self.dmn_dict[self.myaddr]
                    self.raftLock.release('raft_balance')
                    if self.my_daemons is None:
                        self.raft_wait()
                        continue
                    x=False
        self.cluster_aggs = copy.deepcopy(self.aggregators)
        for dmn_grp in list(self.aggregators):
            for dmn in list(self.aggregators[dmn_grp]):
                if dmn not in self.my_daemons:
                    self.aggregators[dmn_grp].pop(dmn)
        for dmn_grp in list(self.aggregators):
            if len(self.aggregators[dmn_grp]) == 0:
                self.aggregators.pop(dmn_grp)
        self.samplers = self.pop_grp(self.samplers)
        self.prdcr_listeners = self.pop_grp(self.prdcr_listeners)
        self.advertisers = self.pop_grp(self.advertisers)
        self.stores = self.pop_grp(self.stores)
        self.producers = self.pop_grp(self.producers)
        self.updaters = self.pop_grp(self.updaters)
        while not self.raftLock.tryAcquire('balance_state', sync=True):
            self.raft_wait()
        self.do_raft_balance.set(self.myaddr, 0, sync=True)
        self.raftLock.release('balance_state')

    def pop_grp(self, spec):
        for dmn_grp in list(spec.keys()):
            grp = 0
            for dmn in self.my_daemons:
                if bin_search(expand_names(dmn_grp), dmn):
                    grp = 1
            if not grp:
                spec.pop(dmn_grp)
        return spec

    def raft_handler(self, cb_fn, *args):
        if self.syncobj:
            cb_fn(*args)

    def raft_check(self):
        if self.syncobj._isLeader():
            last_state = copy.deepcopy(self.maestro_cluster)
            self.maestro_cluster = copy.deepcopy(self.maestro_members)
            cluster_state = self.syncobj.getStatus()
            n = 0
            for mi in self.maestro_members:
                if self.myaddr != f'{mi["host"]}:{mi["port"]}' and cluster_state[f'partner_node_status_server_{mi["host"]}:{mi["port"]}'] != 2:
                    self.maestro_cluster.pop(n)
                    continue
                n += 1
            if last_state != self.maestro_cluster:
                print(f'Raft Balance Detected')
                while not self.raftLock.tryAcquire('balance_state', sync=True):
                    self.raft_wait()
                for mi in self.do_raft_balance.keys():
                    self.do_raft_balance.set(mi, 1, sync=True)
                self.raftLock.release('balance_state')

    def reset_conf(self):
        """
        Re-build the entire config for each of the load balance groups
        """
        self.daemons = copy.deepcopy(self.config[self.prefix]['daemons'])
        self.my_daemons = self.daemons
        self.samplers = copy.deepcopy(self.config[self.prefix].get('samplers', dict()))
        self.producers = copy.deepcopy(self.config[self.prefix].get('producers', dict()))
        self.advertisers = copy.deepcopy(self.config[self.prefix].get('advertisers', dict()))
        self.prdcr_listeners = copy.deepcopy(self.config[self.prefix].get('prdcr_listeners', dict()))
        self.set_aggs()
        self.updaters = copy.deepcopy(self.config[self.prefix].get('updaters', dict()))
        self.stores = copy.deepcopy(self.config[self.prefix].get('stores', dict()))
        self.plugins = copy.deepcopy(self.config[self.prefix].get('plugins', dict()))

    def set_aggs(self):
        if 'aggregators' in self.config[self.prefix]:
            self.aggregators = copy.deepcopy(self.config[self.prefix]['aggregators'])
            for group in self.aggregators:
                for agg in self.aggregators[group]:
                    self.aggregators[group][agg]['prdcrs'] = {}
        else:
            self.aggregators = {}
        self.cluster_aggs = self.aggregators

    def set_cfg_cntr(self):
        for name in self.comms:
            try:
                rc, ldmsd_cfg_cntr = self.comms[name].getCfgCntr()
            except Exception as e:
                print(f'Bad config counter request: {e}')
                ldmsd_cfg_cntr = 0
            self.comms[name].CFG_CNTR = ldmsd_cfg_cntr

    def build_comms(self):
        comms = {}
        for dmn_name in self.daemons:
            listen = []
            hostname = self.daemons[dmn_name]['addr']
            for ep_name in self.daemons[dmn_name]['endpoints']:
                ep = self.daemons[dmn_name]['endpoints'][ep_name]
                hostname = self.daemons[dmn_name]['addr']
                if 'maestro_comm' not in ep:
                    listen.append(ep)
                    continue
                conf = check_opt('conf', ep)
                try:
                    comm = Communicator(ep['xprt'],
                                        hostname,
                                        ep['port'],
                                        ep['auth']['plugin'],
                                        { 'conf' : conf })
                except Exception as e:
                    print(f'Error initializing transport to ldmsd {dmn_name} at endpoint {ep_name} '\
                          f'on port {ep["port"]} with transport {ep["xprt"]}')
                    print(f'{e}')
                    sys.exit()
                rc = comm.connect(timeout=args.timeout)
                if rc:
                    print(f'Error: {rc}')
                    print(f'Failed to communicate with LDMSD {dmn_name} on endpoint {ep["name"]}')
                comms[dmn_name] = comm
                add_listeners(comm, dmn_name, listen)
        return comms

    def check_aggs(self, group, agg_state):
        """
        Build list of responsive aggregators in READY state
        """
        grp_aggs = []
        for agg in self.cluster_aggs[group]:
            if agg not in agg_state[group] and not self.syncobj:
                print(f'Could not connect to {agg}')
                continue
            elif agg_state[group][agg] != 'ready':
                print(f'{agg} not ready, current state {agg_state[group][agg]}')
                continue
            grp_aggs.append(agg)
        return grp_aggs

    def check_samplers(self):
        """
        Build a dict of plugins that have been configured
        for existing samplers.
        """
        cfg_smplrs = {}
        for dmn_grp in self.samplers:
            cfg_smplrs = {}
            for sampler in expand_names(dmn_grp):
                if self.comms[sampler].state == 3:
                    print(f'Unable to connect to ldms daemon {sampler}')
                    continue
                rc, msg = self.comms[sampler].smplr_status()
                msg = fmt_status(msg)
                if rc:
                    if msg is None:
                        print(f'Error {rc}: {sampler} status returned None')
                    else:
                        print(f'Error {rc}: {msg} getting {sampler} status')
                    continue
                elif len(msg) < len(self.samplers[dmn_grp]['plugins']):
                    cfg_smplrs[sampler] = self.daemons[sampler]
                elif len(msg) == len(self.samplers[dmn_grp]['plugins']):
                    for plugin in self.samplers[dmn_grp]['plugins']:
                        print(f'Sampler {plugin} has already been started on {sampler}')
        return cfg_smplrs

    def load_config(self):
        """
        Build a configuration dictionary from the k,v
        in the ETCD store
        """
        pfx = self.prefix
        if pfx[0] != '/':
            pfx = '/' + self.prefix
        config = {}
        try:
            kv_config = self.client.get_prefix(pfx)
        except Exception as e:
            sys.exit(f'Unable to connect to etcd: {e}')
        for v, m in kv_config:
            path = m.key.decode('utf-8')
            s = path.split('/')
            if len(s[0]) == 0:
                # if path begins with '/' strip 1st value
                s = s[1:]
            cfg = config
            for i in range(0, len(s[:-1])):
                d = s[i]
                if d.isdigit():
                    # This is a list entry, append a new dictionary if necessary
                    if len(cfg) <= int(d):
                        cfg.append({})
                    cfg = cfg[int(d)]
                else:
                    if d not in cfg:
                        # look ahead to see if we are adding a dictionary or a list
                        if s[i+1].isdigit():
                            cfg[d] = []
                        else:
                            cfg[d] = {}
                    cfg = cfg[d]
            if isinstance(cfg, list):
                cfg.append(v.decode('utf-8'))
            else:
                cfg[s[-1]] = v.decode('utf-8')
        return config

    def __reconf_endpoints(self, config):
        pass

    def __reconf_daemons(self, config):
        pass

    def __reconf_aggregators(self, config):
        self.set_aggs()

    def __reconf_producers(self, config):
        pass

    def __reconf_samplers(self, config):
        self.res_samplers = True

    def __reconf_stores(self, config):
        pass

    def __reconf_updaters(self, config):
        pass

    def balance_set_remainder(self, group, grp_aggs, grp_sets, rem):
        '''
        Balance metric set remainder
        '''
        rb_aggs = []
        n = len(grp_aggs)
        for agg in reversed(grp_aggs):
            for smplr in list(grp_sets):
                for set_ in list(grp_sets[smplr]['sets']):
                    if agg not in self.aggregators[group]:
                        rem -= grp_sets[smplr]['sets'][set_]
                        grp_sets[smplr]['sets'].pop(set_)
                        continue
                    elif smplr not in self.aggregators[group][agg]['prdcrs']:
                        self.aggregators[group][agg]['prdcrs'][smplr] = {}
                        self.aggregators[group][agg]['prdcrs'][smplr]['sets'] = {}
                    rem -= grp_sets[smplr]['sets'][set_]
                    self.aggregators[group][agg]['prdcrs'][smplr]['sets'][set_] = grp_sets[smplr]['sets'].pop(set_)
                    break
                break
        return rem

    def balance_sets(self, group, grp_aggs, grp_sets, bytes_per_agg):
        '''
        Balance metric sets across aggregators
        '''
        for agg in grp_aggs:
            agg_set_size = 0
            for smplr in list(grp_sets):
                for set_ in list(grp_sets[smplr]['sets']):
                    if agg_set_size + grp_sets[smplr]['sets'][set_] <= bytes_per_agg and grp_sets[smplr]['sets'][set_] != 0:
                        agg_set_size += grp_sets[smplr]['sets'][set_]
                        if group not in self.aggregators or agg not in self.aggregators[group]:
                            grp_sets[smplr]['sets'].pop(set_)
                            continue
                        if smplr not in self.aggregators[group][agg]['prdcrs'] and len(grp_sets[smplr]['sets']) > 0:
                            self.aggregators[group][agg]['prdcrs'][smplr] = {}
                            self.aggregators[group][agg]['prdcrs'][smplr]['sets'] = {}
                        self.aggregators[group][agg]['prdcrs'][smplr]['sets'][set_] = grp_sets[smplr]['sets'].pop(set_)
                if len(grp_sets[smplr]['sets']) == 0:
                    grp_sets.pop(smplr)
            if group in self.aggregators and agg in self.aggregators[group]:
                self.aggregators[group][agg]['agg_set_size'] = agg_set_size
        return grp_sets

    def get_set_weights(self):
        '''
        Check sampler plugins/sets and balance them across aggregators

        Returns (all_sets, dset_size)
            all_sets  - dict of sets organized by group and producer
            dset_size - total size of all sets in group in bytes
        '''
        all_sets = {}
        for group in self.producers:
            # Organize sets by producer group
            dset_size = 0
            all_sets[group] = {}
            all_sets['dset_size'] = dset_size
            all_sets[group]['prdcrs'] = {}
            prdcr_rdy = True
            for prdcr in self.producers[group]:
                if self.producers[group][prdcr]['dmn_grp'] in self.aggregators:
                    continue
                rc, dlist = self.comms[self.producers[group][prdcr]['daemon']].dir_list()
                if rc or dlist is None:
                # Don't inclue sets in group set count if sampler returns error
                    continue
                if len(dlist) == 0:
                    self.do_rebalance = True
                smplr_set_size = 0
                all_sets[group]['prdcrs'][self.producers[group][prdcr]['name']] = {}
                all_sets[group]['prdcrs'][self.producers[group][prdcr]['name']]['sets'] = {}
                for dset in dlist:
                    smplr_set_size += dset.data_size
                    all_sets[group]['prdcrs'][self.producers[group][prdcr]['name']]['sets'][dset.name] = dset.data_size
                all_sets[group]['prdcrs'][self.producers[group][prdcr]['name']]['smplr_set_size'] = smplr_set_size
                dset_size += smplr_set_size
            all_sets[group]['dset_size'] = dset_size
        return all_sets

    def __balance_prdcr_grp(self, group, agg_state):
        if self.syncobj and group not in self.aggregators:
            return 0
        grp_prods = list(self.producers[group].keys())
        grp_aggs = self.check_aggs(group, agg_state)
        if len(grp_aggs) == 0:
            return 1
        dist_prods = dist_list(grp_prods, len(grp_aggs))
        for agg in grp_aggs:
            agg_prods = {}
            for pl in list(dist_prods):
                for p in pl:
                    agg_prods[p] = {}
                dist_prods.pop(0)
                if self.syncobj:
                    if agg not in self.aggregators[group]:
                        break
                self.aggregators[group][agg]['prdcrs'] = agg_prods
                break
        return 0

    def __balance_advertised_prdcrs(self, group, agg_state):
        grp_aggs = []
        for agg in self.ldmsd_state[group]:
            if self.ldmsd_state[group][agg] == 'ready':
                grp_aggs.append(agg)
        if len(grp_aggs) == 0:
            return 1
        for comm in self.comms:
            reconnect = 20
            print(f'Waiting {reconnect} seconds to allow producer listeners time to recognize advertised producers')
            time.sleep(reconnect)
            if self.comms[comm].state != self.comms[comm].CONNECTED:
                self.comms[comm].reconnect()
            err, res = self.comms[comm].prdcr_status()
            if err:
                print(f'Error {err}: getting advertised producers from {comm}: "{res}"')
                continue
            ad_prods = fmt_status(res)
            if not ad_prods:
                continue
            break
        if len(grp_aggs) == 0:
            # None of the aggregators are running
            return 1
        # Assign producers to aggregators
        # Ensure that all aggregator producer listens have successfully seen all advertisers
        while True:
            agg_grp_prods = []
            for agg in grp_aggs:
                while True:
                    err, res = self.comms[agg].prdcr_status()
                    if err and self.ldmsd_state[group][agg] == 'ready':
                        self.comms[agg].reconnect()
                        continue
                    else:
                        break
                agg_grp_prods.append(fmt_status(res))
            agg_iters = iter(agg_grp_prods)
            len_ = len(next(agg_iters))
            if all(len(agg_iter) == len_ for agg_iter in agg_iters):
                del agg_grp_prods
                break
        idx = 0
        for agg in grp_aggs:
            # This code assumes all advertisers in a given aggregator group have been advertised to
            # all aggregators in the group. As producer advertisers, e.g. sampler-1 can not share a name for all aggregators,
            # we must make this assumption in order for the load balancing to operate as expected.
            err, res = self.comms[agg].prdcr_status()
            if err:
                continue
            grp_prods = list(prod['name'] for prod in fmt_status(res))
            agg_prods = {}
            grp_prods.sort()
            dist_prods = dist_list(grp_prods, len(grp_aggs))
            for p in dist_prods[idx]:
                agg_prods[p] = {}
            idx += 1
            if self.syncobj:
                if agg not in self.aggregators[group]:
                    continue
            self.aggregators[group][agg]['prdcrs'] = agg_prods
        return 0

    def __rebalance_prdcrs(self, agg_state=None):
        # Ignore rebalance if this maestro instance has no aggregators
        if not agg_state:
            return
        for group in self.producers:
            rc = self.__balance_prdcr_grp(group, agg_state)
            if rc:
                print(f'Aggregators responsible for producers {group} are not responsive')
        for group in self.prdcr_listeners:
            rc = self.__balance_advertised_prdcrs(group, agg_state)
            if rc:
                print(f'Aggregators responsible for advertisers {group} are not responsive')

    def __rebalance_sets(self, agg_state=None):
        if not agg_state:
            return
        # Build set groups
        all_sets = self.get_set_weights()
        for group in self.producers:
            skip = False
            for prdcr in self.producers[group]:
                if self.producers[group][prdcr]['dmn_grp'] in self.aggregators:
                    skip = True
                break
            if skip:
                self.__balance_prdcr_grp(group, agg_state)
                continue
            dset_size = all_sets[group]['dset_size']
            grp_sets = all_sets[group]['prdcrs']
            grp_aggs = self.check_aggs(group, agg_state)
            agg_count = len(grp_aggs)
            if agg_count != 0:
                size_per_agg = dset_size // agg_count
            else:
                # None of the aggregators are running
                break
            # balance sets across aggregators
            grp_sets =  self.balance_sets(group, grp_aggs, grp_sets, size_per_agg)
            remainder = 0
            if grp_sets:
                for smplr in grp_sets:
                    for set_ in grp_sets[smplr]['sets']:
                        remainder += grp_sets[smplr]['sets'][set_]
            if remainder > 0:
                # Balance each remaining set across aggregators
                remainder = self.balance_set_remainder(group, grp_aggs, grp_sets, remainder)

    def rebalance(self, agg_state):
        self.do_rebalance = False
        # Start samplers if configured with maestro
        cfg_sampler_eps = self.check_samplers()
        if not self.res_samplers:
            if cfg_sampler_eps:
                config_samplers(self.comms, self.plugins, self.samplers, cfg_sampler_eps)
                start_samplers(self.comms, self.plugins, self.samplers, cfg_sampler_eps)
        else:
            if cfg_sampler_eps:
                config_samplers(self.comms, self.plugins, self.samplers, cfg_sampler_eps)
            restart_samplers(self.comms, self.plugins, self.samplers, self.daemons)
            self.res_samplers = False
        # Add advertiser connections
        add_advertisers(self.comms, self.advertisers, self.daemons)
        # Add listener connections
        add_prdcr_listen(self.comms, self.prdcr_listeners, self.aggregators, self.daemons, agg_state)
        print(f'Balancing producers...')
        self.rebalance_handler[self.args.rebalance](agg_state)

        # Add all the producers to the aggregators. Any error here is not an issue
        # because all aggs have all producers, although, not all producers are started.
        add_producers(self.comms, self.producers, self.aggregators, self.daemons, agg_state)
        # Add all the updaters and storage policies
        rc = add_updaters(self.comms, self.updaters, self.aggregators, self.daemons, agg_state, rb=self.args.rebalance)
        if rc:
            self.do_rebalance=True
            return 1
        add_stores(self.comms, self.plugins, self.stores, self.aggregators, self.daemons, agg_state)
        # Start the producers assigned to each of the Aggregators
        start_producers(self.comms, self.aggregators, agg_state)
        stream_subscribe(self.comms, self.producers, self.aggregators, self.daemons, agg_state)
        time.sleep(3)
        self.set_cfg_cntr()
        return 0

    def __etcd_caller(self, event):
        self.__etcd_callback(event.events[0])

    def __etcd_callback(self, event):
        try:
            self.lock.acquire()
            if type(event) != etcd3.events.PutEvent:
                self.lock.release()
                return
            print('Change in etcd cluster configuration.\nReconfiguring...')
            config = self.load_config()
            for conf in self.config[self.prefix]:
                if conf == 'last_updated':
                    continue
                if config[self.prefix][conf] != self.config[self.prefix][conf]:
                    self.etcd_event_handler[conf](config)
                    self.do_rebalance = True
                    break
            self.config = config
            self.reset_conf()
            self.lock.release()
        except Exception as e:
            a, b, c = sys.exc_info()
            print(str(e) + ' ' + str(c.tb_lineno)+'\n')

    def init_mi_state(self):
        if self.syncobj._isLeader():
            while not self.raftLock.tryAcquire('agg_state', sync=True):
                self.raft_wait()
            for dmn_grp in self.cluster_aggs:
                if dmn_grp not in self.ldmsd_mi_state.keys():
                    self.ldmsd_mi_state.addMaestroGroup(dmn_grp, {}, sync=True)
                for dmn in self.cluster_aggs[dmn_grp]:
                    comm = self.comms[dmn]
                    if comm.state != comm.CONNECTED:
                        comm.close()
                        comm.reconnect(timeout=args.timeout)
                    err, msg = comm.daemon_status()
                    if err == 0 and msg is not None:
                        msg = fmt_status(msg)
                        state = msg['state']
                    else:
                        print(f'Error getting daemon {dmn} status')
                        print(f'Error {err}: {msg}')
                        state = 'disconnected'
                    self.ldmsd_mi_state.setDaemon(dmn_grp, dmn, state, sync=True)
        else:
            x = True
            while x==True:
                if self.raftLock.tryAcquire('agg_state', sync=True):
                    test = self.ldmsd_mi_state
                    self.raftLock.release('raft_balance')
                    if len(test) != len(self.cluster_aggs):
                        self.raft_wait()
                        continue
                    x=False
        self.raftLock.release('agg_state')

    def update_mi_state(self):
        while not self.raftLock.tryAcquire('agg_state', sync=True):
            self.raft_wait()
        for dmn_grp in self.cluster_aggs:
            if dmn_grp not in self.aggregators:
                continue
            for dmn in list(self.daemons[dmn_grp]):
                if dmn not in self.aggregators[dmn_grp]:
                    continue
                if self.ldmsd_mi_state[dmn_grp][dmn] != self.ldmsd_state[dmn_grp][dmn]:
                    self.ldmsd_mi_state.setDaemon(dmn_grp, dmn, self.ldmsd_state[dmn_grp][dmn], sync=True)
        self.raftLock.release('agg_state')

    def get_ldmsd_mi_state(self, dmn_grp, dmn):
        self.ldmsd_state[dmn_grp][dmn] = self.ldmsd_mi_state[dmn_grp][dmn]

    def query_ldmsd_state(self):
        self.ldmsd_state = {}
        for grp_name in self.cluster_aggs:
            self.ldmsd_state[grp_name] = {}
            if grp_name not in self.aggregators:
                for name in self.cluster_aggs[grp_name]:
                    self.raft_handler(self.get_ldmsd_mi_state, grp_name, name)
                continue
            for name in self.cluster_aggs[grp_name]:
                if name not in self.aggregators[grp_name]:
                    self.raft_handler(self.get_ldmsd_mi_state, grp_name, name)
                    continue
                comm = self.comms[name]
                if comm.state != comm.CONNECTED:
                    comm.close()
                    comm.reconnect(timeout=args.timeout)
                err, msg = comm.daemon_status()
                if err == 0 and msg is not None:
                    msg = fmt_status(msg)
                    state = msg['state']
                    rc, ldmsd_cfg_cnt = comm.getCfgCntr()
                    if not rc and ldmsd_cfg_cnt != comm.CFG_CNTR:
                        self.do_rebalance = True
                        comm.CFG_CNTR = ldmsd_cfg_cnt
                else:
                    print(f'Error getting daemon {name} status')
                    print(f'Error {err}: {msg}')
                    state = 'disconnected'
                self.ldmsd_state[grp_name][name] = state
        # No aggregators handled by this maestro instance
        if len(self.ldmsd_state) == 0:
            return None
        return self.ldmsd_state

    def cluster_monitor(self):
        last_state = {}
        pfx = '/'+self.prefix
        self.chng_id = self.client.add_watch_callback(pfx+'/last_updated',
                                                      self.__etcd_caller)
        while True:
            self.lock.acquire() # protects self.do_rebalance and self.conf
            self.raft_handler(self.raft_check)
            if self.syncobj:
                if self.do_raft_balance[self.myaddr] != 0:
                    rc = self.raft_balance()
                    self.do_rebalance = True
                    print(f'Finished maesto raft balancing')
            self.query_ldmsd_state()
            if self.ldmsd_state:
                for group in (group for group in self.ldmsd_state if group in self.ldmsd_state):
                    # Updated configuration, do full rebalance
                    if group not in last_state:
                        print(f'New group detected - Rebalance')
                        self.do_rebalance = True
                        last_state = self.ldmsd_state
                        break
                    for dmn in self.ldmsd_state[group]:
                        if dmn not in last_state[group]:
                            print(f'New daemon detected - Rebalance')
                            self.do_rebalance = True
                        elif last_state[group][dmn].lower() != self.ldmsd_state[group][dmn].lower():
                            print(f'Daemon state change - Rebalance')
                            self.raft_handler(self.update_mi_state)
                            self.do_rebalance = True
                        last_state[group][dmn] = self.ldmsd_state[group][dmn]
            if self.do_rebalance:
                print('Rebalance cluster...')
                rc = self.rebalance(self.ldmsd_state)
                print('Finished load balancing.')
            self.lock.release()
            time.sleep(self.args.timeout)

def config_samplers(comms, plugins, samplers, cfg_smplrs):
    """Configure and load plugins for sampler daemons"""
    for smplr in cfg_smplrs:
        for smplr_group in samplers:
            if not bin_search(expand_names(smplr_group), smplr):
                continue
            # Establish communicator to each ldmsd sampler
            comm = comms[smplr]
            err, res = comm.daemon_status()
            if err:
                print(f'config_samplers: lost connectivity to {smplr} err {err}: {res}')
                return False
            print(f'Adding sampler plugins to sampler {smplr}')
            for plugn in samplers[smplr_group]['plugins']:
                plugin = plugins[plugn]
                err, rc = comm.plugn_load(plugn, plugin['name'])
                if err == 17:
                    print(f'File exists: {rc}')
                elif err != 0:
                    print(f'Error {err} loading plugin {plugin["name"]}: {rc}')
                    continue
                # Auto set producer_name/instance/component_id to default,
                # then overwrite if specfied in config
                for cfg_ in plugin['config']:
                    if type(cfg_) is dict:
                        cfg_args = { 'producer'     : smplr,
                                     'instance'     : smplr +'/'+plugn,
                                     'component_id' : '${LDMS_COMPONENT_ID}' }
                        # Set component_id to default env variable if not specified
                        for attr in cfg_:
                            if attr == 'name' or attr == 'interval':
                                continue
                            cfg_args[attr] = cfg_[attr]
                        cfg_str = parse_to_cfg_str(cfg_args)
                    else:
                        cfg_str = cfg_
                    err, msg = comm.plugn_config(plugn, cfg_str)
                    if err:
                        print(f'Error: {err} {msg} configuring {smplr}')
                        continue
    return True

def start_agg_daemons(aggregators, start_aggs):
    if start_aggs:
        for dmn_grp in aggregators:
            for dmn_name in self.daemons[dmn_grp]:
                listen = []
                for ep_name in self.daemons[dmn_grp][dmn_name]['endpoints']:
                    ep = self.daemons[dmn_grp][dmn_name]['endpoints'][ep_name]
                    if 'maestro_comm' not in ep:
                        listen.append(ep)
                        continue
                    if self.args.start_aggregators:
                        # For now create log and pid files in log subdirectory
                        subprocess.run(['ldmsd',
                                        '-x', ep['xprt']+':'+ep['port'],
                                        '-a', ep['auth']['name'],
                                        '-l', 'log/'+dmn_name+'.log',
                                        '-m', '2g',
                                        '-r', 'log/'+dmn_name+'.pid'])

def start_samplers(comms, plugins, samplers, daemons):
    for smplr in daemons:
        for smplr_group in samplers:
            # Establish communicator to each ldmsd sampler
            if not bin_search(expand_names(smplr_group), smplr):
                continue
            comm = comms[smplr]
            err, res = comm.daemon_status()
            if err:
                print(f'start_samplers: lost connectivity to {smplr} err {err}: {res}')
                return False
            smplrs = comm.smplr_status()
            for plugn in samplers[smplr_group]['plugins']:
                plugin = plugins[plugn]
                print(f'Starting.. {plugn} on {smplr}')
                interval = check_opt('interval', plugin)
                offset = check_opt('offset', plugin)
                rc, msg = comm.plugn_start(plugn, interval, offset)
                if rc:
                    print(f'Error {rc} starting {plugn} on {smplr} \n'\
                          f'Error: {msg}')
                    continue
    return True

def stop_samplers(comms, samplers, daemons):
    for smplr in daemons:
        for smplr_group in samplers:
            if not bin_search(expand_names(smplr_group, smplr)):
                continue
            # Establish communicator to each ldmsd sampler
            comm = comms[smplr]
            err, res = comm.daemon_status()
            if err:
                print(f'stop_samplers: lost connectivity to {smplr} err {err}: {res}')
                return False
            smplrs = comm.smplr_status()
            for plugn in smplrs[1]:
                rc, msg = comm.plugn_stop(plugn['name'])
                if rc:
                    print(f'Error {rc} stopping {plugn["name"]} on {smplr} '
                          f'Error: {msg}')
                    continue
                else:
                    print(f'Stopped {plugn["name"]} on {smplr}')
    return True

def restart_samplers(comms, samplers, daemons):
    # Stop all sampler plugins on samplers specified and restart them
    # Does not reconfigure samplers
    rc = stop_samplers(comms, samplers, daemons)
    rc = start_samplers(comms, plugins, samplers, daemons)

def add_listeners(comm, dmn_name, listeners):
    for ep in listeners:
        auth, plugin, auth_opt = check_auth(ep)
        if auth != comm.auth:
            rc, msg = comm.auth_add(auth, plugin, auth_opt)
            if rc:
                print(f'Error adding authentication domain {auth} to sampler {dmn_name}: {msg}')
                if rc == 17:
                    print(f'Authentication domain already exists on sampler {dmn_name}')
                else:
                    print(f'Error code {rc}: {msg}')
                    print(f'Failed to add listener to {ep["xprt"]}:{ep["port"]}')
                    continue
        rc, msg = comm.listen(ep['xprt'], ep['port'], auth=auth)
        if rc:
            print(f'Error adding listening endpoint {ep["name"]}')
            print(f'Error code {rc}: {msg}')

def add_prdcr_listen(comms, prdcr_listen, aggregators, daemons, agg_state):
    """Add listeners to aggregators

    Listeners are added to all aggregators in the group. They
    are in the STANDBY state initially. The "prdcr_listen_start"
    function will start the "advertised" listeners on the aggregator
    that have been assigned by the load balancer.
    """
    for grp_name in prdcr_listen:
        grp_prods = set([ pl for pl in prdcr_listen[grp_name]])
        pl_dict = prdcr_listen[grp_name]
        for agg_name, agg_host in agg_hosts(grp_name, aggregators, daemons):
            if agg_state[grp_name][agg_name] != 'ready' or agg_name not in comms:
                continue
            comm = comms[agg_name]
            for pl in pl_dict:
                rail = check_opt('rail', pl_dict[pl])
                quota = check_opt('quota', pl_dict[pl])
                rx_rate = check_opt('rx_rate', pl_dict[pl])
                regex = check_opt('regex', pl_dict[pl])
                err, res = comm.prdcr_listen_add(pl,
                                                 pl_dict[pl]['reconnect'],
                                                 rail=rail,
                                                 quota=quota,
                                                 rx_rate=rx_rate,
                                                 regex=regex,
                                                 disable_start=True)
                if err:
                    if err == 17:
                        continue
                    print(f'Error {err}: Error adding producer listener to {agg_name}: {res}')
                else:
                    print(f'Add producer listener {pl}')
                err, res = comm.prdcr_listen_start(pl)

def add_advertisers(comms, advertisers, daemons):
    """Adds all aggregators referenced to each producer in the group.

    Each producer will have a connection for each aggregator in the group.
    In the event an aggregator goes down, another aggregator in the group
    will start listening for  the producers that the downed aggregator was
    responsible for
    """
    for grp_name in advertisers:
        ad_dict = advertisers[grp_name]
        ad_names = expand_names(ad_dict['names'])
        for dname in expand_names(grp_name):
            comm = comms[dname]
            auth, plugin, auth_opt = check_auth(ad_dict)
            if auth != comm.auth:
                err, res = comm.auth_add(auth, plugin, auth_opt)
                if err:
                    if err == 17:
                        continue
                    print(f'Error {err} adding authentication domain {auth} to {dname}: {res}')
            for host in expand_names(ad_dict['hosts']):
                ad_name = f'{ad_names[0]}-{host}'
                rail = check_opt('rail', ad_dict)
                quota = check_opt('quota', ad_dict)
                rx_rate = check_opt('rx_rate', ad_dict)
                perm = check_opt('perm', ad_dict)
                err, res = comm.advertiser_add(ad_name,
                                               ad_dict['xprt'],
                                               host,
                                               ad_dict['port'],
                                               ad_dict['reconnect'],
                                               auth=auth,
                                               rail=rail,
                                               quota=quota,
                                               rx_rate=rx_rate,
                                               perm=perm
                )
                if err:
                    if err == 17:
                        continue
                    print(f'Error {err}: Error adding advertiser to {dname}: {res}')
                err, res = comm.advertiser_start(ad_name)
                if err:
                    print(f'Error {err}: Error starting advertiser {dname}: {res}')
                else:
                    print(f'Started advertiser {ad_name} on {dname}')
            ad_names.pop(0)

def add_producers(comms, producers, aggregators, daemons, agg_state):
    """Add producers to Aggregators

    Producers are added to all Aggregators in the group. They
    are all in the STOPPED state initially. The start_producers
    function will start producers on the Aggregator to which
    they have been assigned by the load balancer.

    This should reduce the latency of rebalancing as the
    producer only needs to be started, not added, configured
    and started when it assumes control from a failed peer.
    """
    for grp_name in producers:
        grp_prods = set([ prod for prod in producers[grp_name]])
        prod_dict = producers[grp_name]
        # Query the producer list on each aggregator in the group
        for agg_name, agg_host in agg_hosts(grp_name, aggregators, daemons):
            if agg_state[grp_name][agg_name] != 'ready' or agg_name not in comms:
                continue
            comm = comms[agg_name]
            err, res = comm.prdcr_status()
            prdcrs = fmt_status(res)
            if err:
                print(f'add_producers: lost connectivity to {agg_name} err {err}: {prdcrs}')
                return False
            else:
                agg_config = set({ prod['name'] for prod in prdcrs })
            add_prods = grp_prods - agg_config
            auth = None
            if len(add_prods) > 0:
                print(f'Adding {len(add_prods)} producers to agg {agg_name}')
                ep = next(iter(add_prods))
                dmn_grp = prod_dict[ep]['dmn_grp']
                dmn = prod_dict[ep]['daemon']
                auth, plugin, auth_opt = check_auth(daemons[dmn]['endpoints'][ep])

            if auth is not None:
                rc, msg = comm.auth_add(auth, plugin, auth_opt)
                if rc:
                    print(f'Error adding authentication domain {auth} to aggregator {agg_name}')
                    print(f'Error code {rc}: {msg}')

            for prod_name in add_prods:
                if prod_name not in prod_dict:
                    continue
                if prod_dict[prod_name]['type'] == 'advertised':
                    continue
                prod = prod_dict[prod_name]
                dmn = prod_dict[prod_name]['daemon']
                dmn_grp = prod_dict[prod_name]['dmn_grp']
                ep_name = prod['endpoint']
                endpoint = daemons[dmn]['endpoints'][ep_name]
                hostname = daemons[dmn]['addr']
                perm = check_opt('perm', prod)
                rail = check_opt('rail', prod)
                quota = check_opt('quota', prod)
                rx_rate = check_opt('rx_rate', prod)
                if prod['type'] == 'bridge' or prod['type'] == 'advertiser':
                    prod_name = f'{agg_name}-{endpoint["port"]}'
                comm.prdcr_add(
                            prod_name, prod['type'],
                            endpoint['xprt'], hostname, endpoint['port'],
                            prod['reconnect'], auth=auth, perm=perm, rail=rail,
                            quota=quota, rx_rate=rx_rate)
    return True

def add_updaters(comms, updaters, aggregators, daemons, agg_state, rb=None):
    """
    Add updaters to Aggregators
    """
    rc = 0
    for grp_name in updaters:
        grp_updaters = updaters[grp_name]
        group = aggregators[grp_name]
        for updtr_name in grp_updaters:
            updtr = grp_updaters[updtr_name]
            for agg_name, agg_host in agg_hosts(grp_name, aggregators, daemons):
                if agg_state[grp_name][agg_name] != 'ready' or agg_name not in comms:
                    continue
                agg_updtr = updtr_name + '_' + agg_name
                comm = comms[agg_name]
                # the updater may already exist, in which case the add will fail
                offset = check_opt('offset', updtr)
                perm = check_opt('perm', updtr)
                rc, msg = comm.updtr_status()
                try:
                    prdcr_updtr_status = fmt_status(msg)
                except Exception as e:
                    print(f'Error {e}: loading response {msg} as prdcr updater status from {agg_name}')
                    return 74
                if prdcr_updtr_status is None:
                    print(f'Error {rc}: getting {agg_name} current updater status')
                    print(f'{agg_name} has become unresponsive since last rebalance. Rebalancing...')
                    return 107
                for updtr_ in prdcr_updtr_status:
                    if updtr_['name'] == agg_updtr and updtr_['state'] == 'RUNNING':
                        rc, err = comm.updtr_stop(agg_updtr)
                        if rc:
                            print(f'Error stopping updater {agg_updtr}: {err}')
                        rc, err = comm.updtr_del(agg_updtr)
                        if rc:
                            print(f'Error removing updater {agg_updtr}: {err}')
                if updtr['mode'] == 'auto' or updtr['mode'] == 'auto_interval':
                    rc, msg = comm.updtr_add(agg_updtr, interval=updtr['interval'], offset=offset, auto=True, perm=perm)
                elif updtr['mode'] == 'push':
                    rc, msg = comm.updtr_add(agg_updtr, interval=updtr['interval'], offset=offset, push=True, perm=perm)
                elif updtr['mode'] == 'onchange':
                    rc, msg = comm.updtr_add(agg_updtr, interval=updtr['interval'], offset=offset, push='onchange', perm=perm)
                else:
                    rc, msg = comm.updtr_add(agg_updtr, interval=updtr['interval'], offset=offset, perm=perm)

                if rc:
                    if rc != 17:
                        print(f'Error {rc}: {msg} adding updater {updtr_name} to {agg_host["name"]}')
                for regex in updtr['producers']:
                    comm.updtr_prdcr_add(agg_updtr, regex)
                # Ability for yaml defined set instance matching is removed, as rebalacing is now done
                # across sets, rather than producers
                if rb == 'sets':
                    for prdcr_sets in aggregators[grp_name][agg_name]['prdcrs']:
                        if 'sets' not in aggregators[grp_name][agg_name]['prdcrs'][prdcr_sets]:
                            continue
                        for _set in aggregators[grp_name][agg_name]['prdcrs'][prdcr_sets]['sets']:
                            if 'regex' in updtr['sets']:
                                res = re.match(updtr_['set']['regex'], set_)
                                if not res:
                                    continue
                            err, msg = comm.updtr_match_add(agg_updtr, _set, match='inst')
                            if err:
                                print(f'Error {err} : {msg} matching metric set {_set} to {agg_updtr}')
                err, msg = comm.updtr_start(agg_updtr)
                if err != 0 and err != errno.EBUSY:
                    print(f'Error {err} : {msg} starting {updtr_name}')
                    rc = 1
    return rc

def add_stores(comms, plugins, stores, aggregators, daemons, agg_state):
    """
    Add stores to Aggregators
    """
    for grp_name in stores:
        grp_stores = stores[grp_name]
        for store_name in grp_stores:
            # This will be used when multi-instance plugins are introduced
            loaded_plugins = []
            store = grp_stores[store_name]
            for agg_name, agg_host in agg_hosts(grp_name, aggregators, daemons):
                if agg_state[grp_name][agg_name] != 'ready' or agg_name not in comms:
                    continue
                comm = comms[agg_name]
                store_args = fmt_cmd_args(comm, 'strgp_add', store)
                # Load and configure the plugin required by the store
                plugin_name = store['plugin']
                plugin = plugins[plugin_name]
                store_args['plugin'] = plugin_name
                perm = check_opt('perm', store)
                store_args['perm'] = perm
                err, res = comm.plugn_load(plugin_name, plugin['name'])
                for cfg_ in plugin['config']:
                    if type(cfg_) is dict:
                        cfg_str = parse_to_cfg_str(cfg_)
                    else:
                        cfg_str = cfg_
                    err, msg = comm.plugn_config(plugin_name, cfg_str)
                    if err:
                        print(f'Error {err}: {msg}')
                    else:
                        loaded_plugins.append(plugin_name)

                flush = check_opt('flush', store)
                ## the stores may already exist, in which case the add will fail
                if 'decomp' in store:
                    store_args['decomposition'] = store['decomp']
                err, res = comm.strgp_add(**store_args)
                if err:
                    print(f'Error: {errno.ENOENT} adding storage policy {store_name}: {res}')
                    print(f'Continue without adding "{store_name}"')
                    continue
                ## Add producers
                err, res = comm.strgp_prdcr_add(store_name, ".*")

                # Start the store
                err, msg = comm.strgp_start(store_name)
                if err != 0 and err != errno.EBUSY:
                    print(f'Error {err} : {msg} starting {store_name}')
                    return False
    return True

def start_producers(comms, aggregators, agg_state):
    """
    Query each aggregator for its producer status. If it is already
    running leave it alone, if it is running and not in the new group
    configuration, stop it.
    """
    for grp_name in aggregators:
        agg_grp = aggregators[grp_name]
        for agg in agg_grp:
            if agg_state[grp_name][agg] != 'ready':
                continue
            agg_prods = agg_grp[agg]['prdcrs'].keys()
            comm = comms[agg]

            # get the current state
            err, res = comm.prdcr_status()
            res = fmt_status(res)
            if err:
                print(f'Error {err} querying state from aggregator {agg}: Error {res}')
                continue
            start_prods = list(prod['name'] for prod in res \
                if prod['name'] in agg_prods)
            stop_prods = list(prod['name'] for prod in res \
                if prod['state'] != 'STOPPED' and prod['state'] != 'STANDBY')
            if len(stop_prods) > 0:
                print(f'Stopping agg {agg} {len(stop_prods)} producers')
            for prod_name in stop_prods:
                comm.prdcr_stop(prod_name, regex=False)
            if len(start_prods) > 0:
                print(f'Starting agg {agg} {len(start_prods)} producers')
            for prod_name in start_prods:
                err, res = comm.prdcr_start(prod_name, regex=False)
                if err:
                    print(f'Error {err}: Error starting producer {prod_name} on agg {agg}: {res}')

def stream_subscribe(comms, producers, aggregators, daemons, agg_state):
    """
    Subscribe to streams.
    """
    for grp_name in producers:
        grp_prods = set([ prod for prod in producers[grp_name]])
        prod_dict = producers[grp_name]
        # Query the producer list on each aggregator in the group
        for agg_name, agg_host in agg_hosts(grp_name, aggregators, daemons):
            if agg_state[grp_name][agg_name] != 'ready' or agg_name not in comms:
                continue
            subscribe = check_opt('subscribe', aggregators[grp_name][agg_name])
            if subscribe is None:
                continue
            comm = comms[agg_name]
            err, res = comm.prdcr_status()
            if err:
                print(f'stream_subscribe: lost connectivity to {agg_name} err {err}: {res}')
                return False

            for stream in subscribe:
                if 'regex' in stream:
                    regex = stream['regex']
                else:
                    regex = '.*'
                rx_rate = check_opt('rx_rate', stream)
                if not rx_rate:
                    rx_rate = '-1'
                rc, err = comm.prdcr_subscribe(regex, stream['stream'], rx_rate)
                if rc:
                    print(f'Error {rc} subscribing to stream {stream["stream"]}: {err}.')

RAFT_STATE_TBL = {
        0: "FOLLOWER",
        1: "CANDIDATE",
        2: "LEADER",
    }

def raft_state_str(state):
    return RAFT_STATE_TBL.get(state, f"UNKNOWN_STATE({state})")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="LDMS Monitoring Cluster Manager")
    parser.add_argument("--cluster", metavar="FILE", required=True,
                        help="The name of the etcd cluster configuration file")
    parser.add_argument("--rebalance", metavar="STRING", required=False, default="producers",
                        help='Specify "producers" to balance producers across aggregators,\n'\
                             'or "sets" to balance metric sets across aggregators.\n'\
                             'Defaults to "producers"')
    parser.add_argument("--prefix", metavar="STRING", required=True,
                        help="The prefix for the ldms cluster configurations",
                        default="unknown")
    parser.add_argument("--start-aggregators", action='store_true',
                        help="Start ldms aggregator daemon's")
    parser.add_argument("--timeout", metavar="INT", required=False, type=int,
                        default=15)
    parser.add_argument("--version", metavar="VERSION",
                        help="The OVIS version for the output syntax (4 or 5), default is 4",
                        default=4)
    parser.add_argument("--log-level", default="info",
                        help="Log level (debug, info, warn, error, fatal)")
    parser.add_argument("--debug", action='store_true',
                        help="Enable debug information")
    parser.set_defaults(debug=False)
    args = parser.parse_args()
    if not args.debug:
        import sys
        sys.tracebacklimit=0
    LOG_LEVEL_TBL = dict(
                DEBUG    = logging.DEBUG,
                INFO     = logging.INFO,
                WARN     = logging.WARN,
                WARNING  = logging.WARNING, # alias of WARN
                ERROR    = logging.ERROR,
                FATAL    = logging.FATAL,
                CRITICAL = logging.CRITICAL, # alias of FATAL
            )
    log_level = LOG_LEVEL_TBL.get( args.log_level.upper(), 0 )
    logging.basicConfig(
            format='%(asctime)s.%(msecs)03d %(name)s %(levelname)s %(message)s',
            datefmt='%F %T',
            )
    log = logging.getLogger(__name__)
    log.setLevel(log_level)

    # Load the cluster configuration file. This configures the daemons
    # that support the key/value configuration database
    etcd_fp = open(args.cluster)
    etcd_spec = yaml.safe_load(etcd_fp)

    # All keys in the DB are prefixed with the cluster name, 'pfx'. So we can
    # have multiple monitoring hosted by the same consensus cluster.

    maestro = MaestroMonitor(etcd_spec, args)

    # At each load-balance group level the aggregators have all
    # producers; however, only the producers assigned to each
    # aggregator are started. This practice reduces the latency
    # when an aggregator needs to pick up the load from a
    # failed peer
    maestro.cluster_monitor()
    # evq = queue.Queue()
    # mthread = threading.Thread(target=cluster_monitor)
    # mthread.start()
    # evq.join()

    # config = Cluster(etcd_spec, conf_spec)
    # config.balance()
    # config.commit()
