#!/usr/bin/env python3
import os, sys
import subprocess
import errno
import yaml
import argparse
import etcd3
import json
import threading, queue
import time
import re
import socket
import logging
from ldmsd.ldmsd_communicator import Communicator, fmt_status
from maestro.maestro_util import *

def agg_hosts(grp_name, aggs, daemons):
    return [ [ dmn, daemons[grp_name][dmn]['endpoints'][ep]] for dmn in aggs[grp_name] for ep in daemons[grp_name][dmn]['endpoints'] if 'maestro_comm' in daemons[grp_name][dmn]['endpoints'][ep] ]

class MaestroMonitor(object):
    def __init__(self, spec, args):
        self.prefix = args.prefix
        self.lock = threading.Lock()
        if self.prefix[0] == '/':
            self.prefix = self.prefix[1:]
        self.spec = spec
        # Use the 1st etcd host for now
        etcd_host = spec['members'][0]['host']
        etcd_port = spec['members'][0]['port']
        self.client = etcd3.client(host=etcd_host, port=etcd_port,
                                   grpc_options=[ ('grpc.max_send_message_length',16*1024*1024),
                                                  ('grpc.max_receive_message_length',16*1024*1024)])
        self.syncobj_cond = None
        self.syncobj = None
        self.maestro_members = spec.get('maestro_members')
        if self.maestro_members:
            # maestros on RAFT
            from pysyncobj import SyncObj, SyncObjConf
            self.syncobj_cond = threading.Condition(self.lock)
            myhost = socket.gethostname()
            # sort members by host
            self.maestro_members.sort(key = lambda m: m['host'])
            # Check host uniqueness
            myaddr = None
            otheraddrs = list()
            prev_host = None
            for m in self.maestro_members:
                # sorted by host
                host = m['host']
                port = m['port']
                if host == prev_host:
                    raise RuntimeError(f'{host} appeared more than one time'
                                        ' in the maestro_members list')
                if host == myhost:
                    myaddr = f"{host}:{port}"
                else:
                    otheraddrs.append(f"{host}:{port}")
                prev_host = host
            if not myaddr:
                raise RuntimeError(f'`{myhost}` is not in the maestro_members list')
            syncobjconf = SyncObjConf()
            syncobjconf.onStateChanged = self.on_syncobj_state_changed
            self.syncobj = SyncObj(myaddr, otheraddrs, conf=syncobjconf)
        self.config = self.load_config()
        self.args = args
        if self.args.rebalance != 'sets' and self.args.rebalance != 'producers':
            print(f'"rebalance" parameter must be assigned to either "sets" or "producers"\n'\
                   'If no argument is specified, maestro will balance across producers by default')
            sys.exit(1)
        self.do_rebalance = False
        self.res_samplers = False
        self.reset_conf()
        start_agg_daemons(self.aggregators, self.args.start_aggregators)
        self.comms = self.build_comms()
        self.etcd_event_handler = {
            'endpoints'   : self.__reconf_endpoints,
            'daemons'     : self.__reconf_daemons,
            'aggregators' : self.__reconf_aggregators,
            'producers'   : self.__reconf_producers,
            'samplers'    : self.__reconf_samplers,
            'stores'      : self.__reconf_stores,
            'updaters'    : self.__reconf_updaters
        }
        self.rebalance_handler = {
            'producers' : self.__rebalance_prdcrs,
            'sets'      : self.__rebalance_sets
        }

    def on_syncobj_state_changed(self, old_state, new_state):
        self.lock.acquire()
        log.info(f"syncobj state changed {raft_state_str(old_state)} =>"
                 f" {raft_state_str(new_state)}")
        self.syncobj_cond.notify()
        self.lock.release()

    def reset_conf(self):
        """
        Re-build the entire config for each of the load balance groups
        """
        self.daemons = self.config[self.prefix]['daemons']
        self.set_samplers()
        self.producers = self.config[self.prefix].get('producers', dict())
        self.set_aggs()
        self.updaters = self.config[self.prefix].get('updaters', dict())
        self.stores = self.config[self.prefix].get('stores', dict())

    def set_aggs(self):
        if 'aggregators' in self.config[self.prefix]:
            self.aggregators = self.config[self.prefix]['aggregators']
            for group in self.aggregators:
                for agg in self.aggregators[group]:
                    self.aggregators[group][agg]['prdcrs'] = {}
        else:
            self.aggregators = {}

    def set_samplers(self):
        if 'samplers' in self.config[self.prefix]:
            self.samplers = self.config[self.prefix]['samplers']
        else:
            self.samplers = {}

    def set_cfg_cntr(self):
        for grp_name in self.comms:
            for name in self.comms[grp_name]:
                try:
                    rc, ldmsd_cfg_cntr = self.comms[grp_name][name].getCfgCntr()
                except Exception as e:
                    print(f'Bad config counter request: {e}')
                    ldmsd_cfg_cntr = 0
                self.comms[grp_name][name].CFG_CNTR = ldmsd_cfg_cntr

    def build_comms(self):
        comms = {}
        for dmn_grp in self.daemons:
            comms[dmn_grp] = {}
            for dmn_name in self.daemons[dmn_grp]:
                listen = []
                hostname = self.daemons[dmn_grp][dmn_name]['addr']
                for ep_name in self.daemons[dmn_grp][dmn_name]['endpoints']:
                    ep = self.daemons[dmn_grp][dmn_name]['endpoints'][ep_name]
                    hostname = self.daemons[dmn_grp][dmn_name]['addr']
                    if 'maestro_comm' not in ep:
                        listen.append(ep)
                        continue
                    conf = check_opt('conf', ep)
                    try:
                        comm = Communicator(ep['xprt'],
                                            hostname,
                                            ep['port'],
                                            ep['auth']['plugin'],
                                            { 'conf' : conf })
                    except Exception as e:
                        print(f'Error initializing transport to ldmsd {dmn_name} at endpoint {ep_name} '\
                              f'on port {ep["port"]} with transport {ep["xprt"]}')
                        print(f'{e}')
                        sys.exit()
                    rc = comm.connect(timeout=args.timeout)
                    if rc:
                        print(f'Error: {rc}')
                        print(f'Failed to communicate with aggregator {dmn_name} on endpoint {ep["name"]}')
                    comms[dmn_grp][dmn_name] = comm
                    add_listeners(comm, dmn_name, listen)
        return comms

    def check_aggs(self, group, agg_state):
        """
        Build list of responsive aggregators in READY state
        """
        grp_aggs = []
        for agg in self.aggregators[group]:
            if agg not in agg_state[group]:
                print(f'Could not connect to {agg}')
                continue
            elif agg_state[group][agg] != 'ready':
                print(f'{agg} not ready, current state {agg_state[group][agg]}')
                continue
            grp_aggs.append(agg)
        return grp_aggs

    def check_samplers(self):
        """
        Build a dict of plugins that have been configured
        for existing samplers.
        """
        cfg_smplrs = {}
        for dmn_grp in self.samplers:
            cfg_smplrs[dmn_grp] = {}
            for sampler in self.comms[dmn_grp]:
                if self.comms[dmn_grp][sampler].state == 3:
                    print(f'Unable to connect to ldms daemon {sampler}')
                    continue
                rc, msg = self.comms[dmn_grp][sampler].smplr_status()
                msg = fmt_status(msg)
                if rc:
                    if msg is None:
                        print(f'Error {rc}: {sampler} status returned None')
                    else:
                        print(f'Error {rc}: {msg} getting {sampler} status')
                    continue
                elif len(msg) < len(self.samplers[dmn_grp]['plugins']):
                    cfg_smplrs[dmn_grp][sampler] = self.daemons[dmn_grp][sampler]
                elif len(msg) == len(self.samplers[dmn_grp]['plugins']):
                    for plugin in self.samplers[dmn_grp]['plugins']:
                        print(f'Sampler {plugin["name"]} has already been started on {sampler}')
        return cfg_smplrs

    def load_config(self):
        """
        Build a configuration dictionary from the k,v
        in the ETCD store
        """
        pfx = self.prefix
        if pfx[0] != '/':
            pfx = '/' + self.prefix
        config = {}
        try:
            kv_config = self.client.get_prefix(pfx)
        except Exception as e:
            sys.exit(f'Unable to connect to etcd: {e}')
        for v, m in kv_config:
            path = m.key.decode('utf-8')
            s = path.split('/')
            if len(s[0]) == 0:
                # if path begins with '/' strip 1st value
                s = s[1:]
            cfg = config
            for i in range(0, len(s[:-1])):
                d = s[i]
                if d.isdigit():
                    # This is a list entry, append a new dictionary if necessary
                    if len(cfg) <= int(d):
                        cfg.append({})
                    cfg = cfg[int(d)]
                else:
                    if d not in cfg:
                        # look ahead to see if we are adding a dictionary or a list
                        if s[i+1].isdigit():
                            cfg[d] = []
                        else:
                            cfg[d] = {}
                    cfg = cfg[d]
            if isinstance(cfg, list):
                cfg.append(v.decode('utf-8'))
            else:
                cfg[s[-1]] = v.decode('utf-8')
        return config

    def __reconf_endpoints(self, config):
        pass

    def __reconf_daemons(self, config):
        pass

    def __reconf_aggregators(self, config):
        self.set_aggs()

    def __reconf_producers(self, config):
        pass

    def __reconf_samplers(self):
        self.res_samplers = True

    def __reconf_stores(self, config):
        pass

    def __reconf_updaters(self, config):
        pass

    def balance_remainder(self, group, grp_aggs, grp_sets, rem):
        rb_aggs = []
        n = len(grp_aggs)
        for agg in reversed(grp_aggs):
            for smplr in list(grp_sets):
                for set_ in list(grp_sets[smplr]['sets']):
                    if smplr not in self.aggregators[group][agg]['prdcrs']:
                        self.aggregators[group][agg]['prdcrs'][smplr] = {}
                        self.aggregators[group][agg]['prdcrs'][smplr]['sets'] = {}
                    rem -= grp_sets[smplr]['sets'][set_]
                    self.aggregators[group][agg]['prdcrs'][smplr]['sets'][set_] = grp_sets[smplr]['sets'].pop(set_)
                    break
                break
        return rem

    def balance_sets(self, group, grp_aggs, grp_sets, bytes_per_agg):
        for agg in grp_aggs:
            agg_set_size = 0
            for smplr in list(grp_sets):
                for set_ in list(grp_sets[smplr]['sets']):
                    if agg_set_size + grp_sets[smplr]['sets'][set_] <= bytes_per_agg and grp_sets[smplr]['sets'][set_] != 0:
                        agg_set_size += grp_sets[smplr]['sets'][set_]
                        if smplr not in self.aggregators[group][agg]['prdcrs'] and len(grp_sets[smplr]['sets']) > 0:
                            self.aggregators[group][agg]['prdcrs'][smplr] = {}
                            self.aggregators[group][agg]['prdcrs'][smplr]['sets'] = {}
                        self.aggregators[group][agg]['prdcrs'][smplr]['sets'][set_] = grp_sets[smplr]['sets'].pop(set_)
                if len(grp_sets[smplr]['sets']) == 0:
                    grp_sets.pop(smplr)
            self.aggregators[group][agg]['agg_set_size'] = agg_set_size
        return grp_sets

    def get_set_weights(self):
        '''
        Check sampler plugins/sets and balance them across aggregators

        Returns (all_sets, dset_size)
            all_sets  - dict of sets organized by group and producer
            dset_size - total size of all sets in group in bytes
        '''
        all_sets = {}
        for group in self.producers:
            # Organize sets by producer group
            dset_size = 0
            all_sets[group] = {}
            all_sets['dset_size'] = dset_size
            all_sets[group]['prdcrs'] = {}
            for prdcr in self.producers[group]:
                rc, dlist = self.comms[self.producers[group][prdcr]['dmn_grp']][self.producers[group][prdcr]['daemon']].dir_list()
                if rc or dlist is None:
                # Don't inclue sets in group set count if sampler returns error
                    continue
                smplr_set_size = 0
                all_sets[group]['prdcrs'][self.producers[group][prdcr]['name']] = {}
                all_sets[group]['prdcrs'][self.producers[group][prdcr]['name']]['sets'] = {}
                for dset in dlist:
                    smplr_set_size += dset.data_size
                    all_sets[group]['prdcrs'][self.producers[group][prdcr]['name']]['sets'][dset.name] = dset.data_size
                all_sets[group]['prdcrs'][self.producers[group][prdcr]['name']]['smplr_set_size'] = smplr_set_size
                dset_size += smplr_set_size
            all_sets[group]['dset_size'] = dset_size
        return all_sets

    def __rebalance_prdcrs(self, agg_state):
        for group in self.producers:
            grp_prods = list(self.producers[group].keys())
            prod_count = len(grp_prods)
            grp_aggs = self.check_aggs(group, agg_state)
            agg_count = len(grp_aggs)
            if agg_count != 0:
                prods_per_agg = prod_count // agg_count
                remainder = prod_count % agg_count
            else:
                # None of the aggregators are running
                break
            # Assign producers to aggregators
            prod_idx = 0
            for agg in grp_aggs:
                agg_prods = {}
                for cnt in range(0, prods_per_agg):
                    agg_prods[grp_prods[prod_idx]] = {}
                    prod_idx += 1
                if remainder > 0:
                    agg_prods[grp_prods[prod_idx]] = {}
                    prod_idx += 1
                    remainder -= 1
                self.aggregators[group][agg]['prdcrs'] = agg_prods

    def __rebalance_sets(self, agg_state):
        # Build set groups
        all_sets = self.get_set_weights()
        for group in self.producers:
            dset_size = all_sets[group]['dset_size']
            grp_sets = all_sets[group]['prdcrs']
            grp_aggs = self.check_aggs(group, agg_state)
            agg_count = len(grp_aggs)
            if agg_count != 0:
                size_per_agg = dset_size // agg_count
            else:
                # None of the aggregators are running
                break
            # balance sets across aggregators
            grp_sets =  self.balance_sets(group, grp_aggs, grp_sets, size_per_agg)
            remainder = 0
            if grp_sets:
                for smplr in grp_sets:
                    for set_ in grp_sets[smplr]['sets']:
                        remainder += grp_sets[smplr]['sets'][set_]
            if remainder > 0:
                # Balance each remaining set across aggregators
                remainder = self.balance_remainder(group, grp_aggs, grp_sets, remainder)

    def rebalance(self, agg_state):
        self.reset_conf()
        # Start samplers if configured with maestro
        cfg_sampler_eps = self.check_samplers()
        if not self.res_samplers:
            if cfg_sampler_eps:
                config_samplers(self.comms, self.samplers, cfg_sampler_eps)
                start_samplers(self.comms, self.samplers, cfg_sampler_eps)
        else:
            if cfg_sampler_eps:
                config_samplers(self.comms, self.samplers, cfg_sampler_eps)
            restart_samplers(self.comms, self.samplers, self.daemons)
            self.res_samplers = False

        self.rebalance_handler[self.args.rebalance](agg_state)

        # Add all the producers to the aggregators. Any error here is not an issue
        # because all aggs have all producers, although, not all producers are started.
        add_producers(self.comms, self.producers, self.aggregators, self.daemons, agg_state)

        # Add all the updaters and storage policies
        rc = add_updaters(self.comms, self.updaters, self.aggregators, self.daemons, agg_state, rb=self.args.rebalance)
        if rc:
            self.do_balance=True
            return 1
        add_stores(self.comms, self.stores, self.aggregators, self.daemons, agg_state)
        # Start the producers assigned to each of the Aggregators
        start_producers(self.comms, self.aggregators, agg_state)
        stream_subscribe(self.comms, self.producers, self.aggregators, self.daemons, agg_state)
        self.set_cfg_cntr()
        return 0

    def __etcd_caller(self, event):
        self.__etcd_callback(event.events[0])

    def __etcd_callback(self, event):
        try:
            self.lock.acquire()
            if type(event) != etcd3.events.PutEvent:
                self.lock.release()
                return
            print('Change in etcd cluster configuration.\nReconfiguring...')
            config = self.load_config()
            for conf in self.config[self.prefix]:
                if conf == 'last_updated':
                    continue
                if config[self.prefix][conf] != self.config[self.prefix][conf]:
                    self.etcd_event_handler[conf]()
                    self.do_rebalance = True
                    break
            self.config = config
            self.lock.release()
        except Exception as e:
            a, b, c = sys.exc_info()
            print(str(e) + ' ' + str(c.tb_lineno)+'\n')

    def query_ldmsd_state(self):
        ldmsd_state = {}
        for grp_name in self.comms:
            group = self.comms[grp_name]
            for name in group:
                if grp_name not in ldmsd_state:
                    ldmsd_state[grp_name] = {}
                comm = group[name]
                if comm.state != comm.CONNECTED:
                    comm.close()
                    comm.reconnect(timeout=args.timeout)
                err, msg = comm.daemon_status()
                if err == 0 and msg is not None:
                    msg = fmt_status(msg)
                    state = msg['state']
                    rc, ldmsd_cfg_cnt = comm.getCfgCntr()
                    if not rc and ldmsd_cfg_cnt != comm.CFG_CNTR:
                        self.do_rebalance = True
                        comm.CFG_CNTR = ldmsd_cfg_cnt
                else:
                    print(f'Error getting daemon {name} status')
                    print(f'Error {err}: {msg}')
                    state = 'disconnected'
                ldmsd_state[grp_name][name] = state
        return ldmsd_state

    def cluster_monitor(self):
        last_state = {}
        pfx = '/'+self.prefix
        self.chng_id = self.client.add_watch_callback(pfx+'/last_updated',
                                                      self.__etcd_caller)
        while True:
            self.lock.acquire() # protects self.do_rebalance and self.conf
            if self.syncobj:
                # maestros on RAFT, only act if we are the leader
                while not self.syncobj._isLeader():
                    self.syncobj_cond.wait()
                    # NOTE: cond.wait() releases self.lock and blocking waits.
                    #       self.lock is re-acquired when cond.wait() returns.

            ldmsd_state = self.query_ldmsd_state()
            rebalance_aggs = {}
            agg_str = ''
            for group in ldmsd_state:
                # Updated configuration, do full rebalance
                if group not in last_state:
                    self.do_rebalance = True
                    last_state = ldmsd_state
                    break
                for dmn in ldmsd_state[group]:
                    if dmn not in last_state[group]:
                        self.do_rebalance = True
                    elif last_state[group][dmn].lower() != ldmsd_state[group][dmn].lower():
                        self.do_rebalance = True
                    last_state[group][dmn] = ldmsd_state[group][dmn]
            if self.do_rebalance:
                print('Rebalance cluster...')
                rc = self.rebalance(ldmsd_state)
                self.do_rebalance = False
                print('Finished load balancing.')
            self.lock.release()
            time.sleep(1)

def config_samplers(comms, samplers, daemons):
    """Configure and load plugins for sampler daemons"""
    for smplr_group in samplers:
        for smplr in daemons[smplr_group]:
            smplr_host = daemons[smplr_group][smplr]
            # Establish communicator to each ldmsd sampler
            comm = comms[samplers[smplr_group]['daemons']][smplr]
            err, res = comm.daemon_status()
            if err:
                print(f'config_samplers: lost connectivity to {smplr} err {err}: {res}')
                return False
            print(f'Adding sampler plugins to sampler {smplr}')
            for plugin in samplers[smplr_group]['plugins']:
                err, rc = comm.plugn_load(plugin['name'])
                if err == 17:
                    print(f'File exists: {rc}')
                elif err != 0:
                    print(f'Error {err} loading plugin {plugin["name"]}: {rc}')
                    continue
                # Auto set producer_name/instance/component_id to default,
                # then overwrite if specfied in config
                cfg_args = { 'producer'     : smplr,
                             'instance'     : smplr +'/'+plugin['name'],
                             'component_id' : '${LDMS_COMPONENT_ID}' }
                for attr in plugin['config']:
                    if attr == 'name' or attr == 'interval':
                        continue
                    cfg_args[attr] = plugin['config'][attr]
                # Set component_id to default env variable if not specified
                cfg_str = parse_to_cfg_str(cfg_args)
                err, msg = comm.plugn_config(plugin['name'], cfg_str)
                if err:
                    print(f'Error: {err} {msg} configuring {smplr}')
                    continue
    return True

def start_agg_daemons(aggregators, start_aggs):
    if start_aggs:
        for dmn_grp in aggregators:
            comms[dmn_grp] = {}
            for dmn_name in self.daemons[dmn_grp]:
                listen = []
                for ep_name in self.daemons[dmn_grp][dmn_name]['endpoints']:
                    ep = self.daemons[dmn_grp][dmn_name]['endpoints'][ep_name]
                    if 'maestro_comm' not in ep:
                        listen.append(ep)
                        continue
                    if self.args.start_aggregators:
                        # For now create log and pid files in log subdirectory
                        subprocess.run(['ldmsd',
                                        '-x', ep['xprt']+':'+ep['port'],
                                        '-a', ep['auth']['name'],
                                        '-l', 'log/'+dmn_name+'.log',
                                        '-m', '2g',
                                        '-r', 'log/'+dmn_name+'.pid'])

def start_samplers(comms, samplers, daemons):
    for smplr_group in samplers:
        for smplr in daemons[smplr_group]:
            smplr_host = daemons[smplr_group][smplr]
            # Establish communicator to each ldmsd sampler
            comm = comms[samplers[smplr_group]['daemons']][smplr]
            err, res = comm.daemon_status()
            if err:
                print(f'start_samplers: lost connectivity to {smplr} err {err}: {res}')
                return False
            smplrs = comm.smplr_status()
            for plugin in samplers[smplr_group]['plugins']:
                print(f'Starting.. {plugin["name"]} on {smplr}')
                interval = check_opt('interval', plugin)
                offset = check_opt('offset', plugin)
                rc, msg = comm.plugn_start(plugin['name'], interval, offset)
                if rc:
                    print(f'Error {rc} starting {plugin["name"]} on {smplr} \n'\
                          f'Error: {msg}')
                    continue
    return True

def stop_samplers(comms, samplers, daemons):
    for smplr_group in samplers:
        for smplr in daemons[smplr_group]:
            smplr_host = daemons[smplr_group][smplr]
            # Establish communicator to each ldmsd sampler
            comm = comms[samplers[smplr_group]['daemons']][smplr]
            err, res = comm.daemon_status()
            if err:
                print(f'stop_samplers: lost connectivity to {smplr} err {err}: {res}')
                return False
            smplrs = comm.smplr_status()
            for plugn in smplrs[1]:
                rc, msg = comm.plugn_stop(plugn['name'])
                if rc:
                    print(f'Error {rc} stopping {plugn["name"]} on {smplr} '
                          f'Error: {msg}')
                    continue
                else:
                    print(f'Stopped {plugn["name"]} on {smplr}')
    return True

def restart_samplers(comms, samplers, daemons):
    # Stop all sampler plugins on samplers specified and restart them
    # Does not reconfigure samplers
    rc = stop_samplers(comms, samplers, daemons)
    rc = start_samplers(comms, samplers, daemons)

def add_listeners(comm, dmn_name, listeners):
    for ep in listeners:
        auth = check_opt('auth', ep)
        auth_opt = check_opt('conf', ep)
        plugin = check_opt('plugin', ep['auth'])
        if auth != comm.auth:
            rc, msg = comm.auth_add(auth, plugin, auth_opt)
            if rc:
                print(f'Error adding authentication domain {auth} to sampler {dmn_name}: {msg}')
                if rc == 17:
                    print(f'Authentication domain already exists on sampler {dmn_name}')
                else:
                    print(f'Error code {rc}: {msg}')
                    print(f'Failed to add listener to {ep["xprt"]}:{ep["port"]}')
                    continue
        rc, msg = comm.listen(ep['xprt'], ep['port'], auth=auth)
        if rc:
            print(f'Error adding listening endpoint {ep["name"]}')
            print(f'Error code {rc}: {msg}')

def add_producers(comms, producers, aggregators, endpoints, agg_state):
    """Add producers to Aggregators

    Producers are added to all Aggregators in the group. They
    are all in the STOPPED state initially. The start_producers
    function will start producers on the Aggregator to which
    they have been assigned by the load balancer.

    This should reduce the latency of rebalancing as the
    producer only needs to be started, not added, configured
    and started when it assumes control from a failed peer.
    """
    for grp_name in producers:
        grp_prods = set([ prod for prod in producers[grp_name]])
        prod_dict = producers[grp_name]
        # Query the producer list on each aggregator in the group
        for agg_name, agg_host in agg_hosts(grp_name, aggregators, endpoints):
            if agg_state[grp_name][agg_name] != 'ready':
                continue
            comm = comms[grp_name][agg_name]
            err, res = comm.prdcr_status()
            prdcrs = fmt_status(res)
            if err:
                print(f'add_producers: lost connectivity to {agg_name} err {err}: {prdcrs}')
                return False
            else:
                agg_config = set({ prod['name'] for prod in prdcrs })
            add_prods = grp_prods - agg_config
            auth = None
            if len(add_prods) > 0:
                print(f'Adding {len(add_prods)} producers to agg {agg_name}')
                ep = next(iter(add_prods))
                dmn_grp = prod_dict[ep]['dmn_grp']
                dmn = prod_dict[ep]['daemon']
                auth = check_opt('auth', endpoints[dmn_grp][dmn]['endpoints'][ep])
                auth_opt = check_opt('conf', endpoints[dmn_grp][dmn]['endpoints'][ep])
                plugin = check_opt('plugin', endpoints[dmn_grp][dmn]['endpoints'][ep]['auth'])

            if auth is not None:
                rc, msg = comm.auth_add(auth, plugin, auth_opt)
                if rc:
                    print(f'Error adding authentication domain {auth} to aggregator {agg_name}')
                    print(f'Error code {rc}: {msg}')

            for prod_name in add_prods:
                if prod_name not in prod_dict:
                    continue
                prod = prod_dict[prod_name]
                dmn = prod_dict[prod_name]['daemon']
                dmn_grp = prod_dict[prod_name]['dmn_grp']
                ep_name = prod['endpoint']
                endpoint = endpoints[dmn_grp][dmn]['endpoints'][ep_name]
                hostname = endpoints[dmn_grp][dmn]['addr']
                auth = check_opt('auth', endpoint)
                comm.prdcr_add(
                            prod_name, prod['type'],
                            endpoint['xprt'], hostname, endpoint['port'],
                            prod['reconnect'], auth=auth)
    return True

def add_updaters(comms, updaters, aggregators, daemons, agg_state, rb=None):
    """
    Add updaters to Aggregators
    """
    rc = 0
    for grp_name in updaters:
        grp_updaters = updaters[grp_name]
        group = aggregators[grp_name]
        for updtr_name in grp_updaters:
            updtr = grp_updaters[updtr_name]
            for agg_name, agg_host in agg_hosts(grp_name, aggregators, daemons):
                if agg_state[grp_name][agg_name] != 'ready':
                    continue
                agg_updtr = updtr_name + '_' + agg_name
                comm = comms[grp_name][agg_name]
                # the updater may already exist, in which case the add will fail
                offset = check_opt('offset', updtr)
                rc, msg = comm.updtr_status()
                try:
                    prdcr_updtr_status = fmt_status(msg)
                except Exception as e:
                    print(f'Error {e}: loading response {msg} as prdcr updater status from {agg_name}')
                    return 74
                if prdcr_updtr_status is None:
                    print(f'Error {err}: getting {agg_name} current updater status')
                    print(f'{agg_name} has become unresponsive since last rebalance. Rebalancing...')
                    return 107
                for updtr_ in prdcr_updtr_status:
                    if updtr_['name'] == agg_updtr and updtr_['state'] == 'RUNNING':
                        rc, err = comm.updtr_stop(agg_updtr)
                        if rc:
                            print(f'Error stopping updater {agg_updtr}: {err}')
                        rc, err = comm.updtr_del(agg_updtr)
                        if rc:
                            print(f'Error removing updater {agg_updtr}: {err}')
                if updtr['mode'] == 'auto' or updtr['mode'] == 'auto_interval':
                    rc, msg = comm.updtr_add(agg_updtr, interval=updtr['interval'], offset=offset, auto=True)
                elif updtr['mode'] == 'push':
                    rc, msg = comm.updtr_add(agg_updtr, interval=updtr['interval'], offset=offset, push=True)
                elif updtr['mode'] == 'onchange':
                    rc, msg = comm.updtr_add(agg_updtr, interval=updtr['interval'], offset=offset, push='onchange')
                else:
                    rc, msg = comm.updtr_add(agg_updtr, interval=updtr['interval'], offset=offset)

                if rc:
                    if rc != 17:
                        print(f'Error {rc}: {msg} adding updater {updtr_name} to {agg_host["name"]}')
                for regex in updtr['producers']:
                    comm.updtr_prdcr_add(agg_updtr, regex['regex'])
                # Ability for yaml defined set instance matching is removed, as rebalacing is now done
                # across sets, rather than producers
                if rb == 'sets':
                    for prdcr_sets in aggregators[grp_name][agg_name]['prdcrs']:
                        for _set in aggregators[grp_name][agg_name]['prdcrs'][prdcr_sets]['sets']:
                            if 'regex' in updtr['sets']:
                                res = re.match(updtr_['set']['regex'], set_)
                                if not res:
                                    continue
                            err, msg = comm.updtr_match_add(agg_updtr, _set, match='inst')
                            if err:
                                print(f'Error {err} : {msg} matching metric set {_set} to {agg_updtr}')
                err, msg = comm.updtr_start(agg_updtr)
                if err != 0 and err != errno.EBUSY:
                    print(f'Error {err} : {msg} starting {updtr_name}')
                    rc = 1
    return rc

def add_stores(comms, stores, aggregators, daemons, agg_state):
    """
    Add stores to Aggregators
    """
    for grp_name in stores:
        grp_stores = stores[grp_name]
        for store_name in grp_stores:
            store = grp_stores[store_name]
            for agg_name, agg_host in agg_hosts(grp_name, aggregators, daemons):
                if agg_state[grp_name][agg_name] != 'ready':
                    continue
                comm = comms[grp_name][agg_name]
                # Load and configure the plugin required by the store
                plugin = store['plugin']
                plugin_name = plugin['name']
                cfg_str = parse_to_cfg_str(plugin['config'])
                err, res = comm.plugn_load(plugin_name)
                err, res = comm.plugn_config(plugin_name, cfg_str)
                flush = check_opt('flush', store)
                ## the stores may already exist, in which case the add will fail
                err, res = comm.strgp_add(store_name,
                                          plugin_name,
                                          store['container'],
                                          store['schema'],
                                          flush=flush)
                ## Add producers
                err, res = comm.strgp_prdcr_add(store_name, ".*")

                # Start the store
                err, msg = comm.strgp_start(store_name)
                if err != 0 and err != errno.EBUSY:
                    print(f'Error {err} : {msg} starting {store_name}')
                    return False
    return True

def start_producers(comms, groups, agg_state):
    """
    Query each aggregator for its producer status. If it is already
    running leave it alone, if it is running and not in the new group
    configuration, stop it.
    """
    for grp_name in groups:
        group = groups[grp_name]
        for agg in group:
            if agg_state[grp_name][agg] != 'ready':
                continue
            agg_prods = group[agg]['prdcrs'].keys()
            comm = comms[grp_name][agg]

            # get the current state
            err, res = comm.prdcr_status()
            res = fmt_status(res)
            if err:
                print(f'Error {err} querying state from aggregator {agg}: Error {res}')
                continue
            start_prods = list(prod['name'] for prod in res \
                if prod['name'] in agg_prods)
            stop_prods = list(prod['name'] for prod in res \
                if prod['state'] != 'STOPPED')
            if len(stop_prods) > 0:
                print(f'Stopping agg {agg} {len(stop_prods)} producers')
            for prod_name in stop_prods:
                comm.prdcr_stop(prod_name, regex=False)
            for prod_name in start_prods:
                comm.prdcr_start(prod_name, regex=False)
            if len(start_prods) > 0:
                print(f'Starting agg {agg} {len(start_prods)} producers')

def stream_subscribe(comms, producers, aggregators, endpoints, agg_state):
    """
    Subscribe to streams.
    """
    for grp_name in producers:
        grp_prods = set([ prod for prod in producers[grp_name]])
        prod_dict = producers[grp_name]
        # Query the producer list on each aggregator in the group
        for agg_name, agg_host in agg_hosts(grp_name, aggregators, endpoints):
            if agg_state[grp_name][agg_name] != 'ready':
                continue
            comm = comms[grp_name][agg_name]
            err, res = comm.prdcr_status()
            if err:
                print(f'stream_subscribe: lost connectivity to {agg_name} err {err}: {res}')
                return False

            subscribe = check_opt('subscribe', aggregators[grp_name][agg_name])
            if subscribe is not None:
                for stream in subscribe:
                    if 'regex' in stream:
                        regex = stream['regex']
                    else:
                        regex = '.*'
                    rc, err = comm.prdcr_subscribe(regex, stream['stream'])
                    if rc:
                        print(f'Error {rc} subscribing to stream {stream["stream"]}: {err}.')

RAFT_STATE_TBL = {
        0: "FOLLOWER",
        1: "CANDIDATE",
        2: "LEADER",
    }

def raft_state_str(state):
    return RAFT_STATE_TBL.get(state, f"UNKNOWN_STATE({state})")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="LDMS Monitoring Cluster Manager")
    parser.add_argument("--cluster", metavar="FILE", required=True,
                        help="The name of the etcd cluster configuration file")
    parser.add_argument("--rebalance", metavar="STRING", required=False, default="producers",
                        help='Specify "producers" to balance producers across aggregators,\n'\
                             'or "sets" to balance metric sets across aggregators.\n'\
                             'Defaults to "producers"')
    #parser.add_argument("--dump", action="store_true",
    #                    help="Dump the load-balanced aggregation configuration files")

    parser.add_argument("--prefix", metavar="STRING", required=True,
                        help="The prefix for the ldms cluster configurations",
                        default="unknown")
    parser.add_argument("--start-aggregators", action='store_true',
                        help="Start ldms aggregator daemon's")
    parser.add_argument("--timeout", metavar="INT", required=False, type=int,
                        default=15)
    parser.add_argument("--version", metavar="VERSION",
                        help="The OVIS version for the output syntax (4 or 5), default is 4",
                        default=4)
    parser.add_argument("--log-level", default="info",
                        help="Log level (debug, info, warn, error, fatal)")
    args = parser.parse_args()
    LOG_LEVEL_TBL = dict(
                DEBUG    = logging.DEBUG,
                INFO     = logging.INFO,
                WARN     = logging.WARN,
                WARNING  = logging.WARNING, # alias of WARN
                ERROR    = logging.ERROR,
                FATAL    = logging.FATAL,
                CRITICAL = logging.CRITICAL, # alias of FATAL
            )
    log_level = LOG_LEVEL_TBL.get( args.log_level.upper(), 0 )
    logging.basicConfig(
            format='%(asctime)s.%(msecs)03d %(name)s %(levelname)s %(message)s',
            datefmt='%F %T',
            )
    log = logging.getLogger(__name__)
    log.setLevel(log_level)

    # Load the cluster configuration file. This configures the daemons
    # that support the key/value configuration database
    etcd_fp = open(args.cluster)
    etcd_spec = yaml.safe_load(etcd_fp)

    # All keys in the DB are prefixed with the cluster name, 'pfx'. So we can
    # have multiple monitoring hosted by the same consensus cluster.

    maestro = MaestroMonitor(etcd_spec, args)

    # At each load-balance group level the aggregators have all
    # producers; however, only the producers assigned to each
    # aggregator are started. This practice reduces the latency
    # when an aggregator needs to pick up the load from a
    # failed peer
    maestro.cluster_monitor()
    # evq = queue.Queue()
    # mthread = threading.Thread(target=cluster_monitor)
    # mthread.start()
    # evq.join()

    # config = Cluster(etcd_spec, conf_spec)
    # config.balance()
    # config.commit()
    # if args.dump:
    #    config.dump_config(args.prefix, args.version)
