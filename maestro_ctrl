#!/usr/bin/env python3
import os, sys
import errno
import yaml
import hostlist
import collections
import argparse
import json
import time
from collections import Mapping, Sequence
from noetcd import NoetcdClient
from noetcd import MaestroBooleanOptionalAction

class Cluster(object):
    def emit_value(self, path, value):
        try:
            res = client.put(path, str(value))
        except Exception as e:
            print("Error {0} setting {1} : {2}".format(str(e), path, str(value)))

    def walk(self, obj, path=''):
        if isinstance(obj, Mapping):
            for key in obj:
                self.walk(obj[key], '{0}/{1}'.format(path, str(key)))
        elif isinstance(obj, Sequence):
            if isinstance(obj, (str, bytearray)):
                self.emit_value(path, obj)
            else:
                item = 0
                for v in obj:
                    # we want keys to be returned in numerical order which requires z-fill
                    self.walk(v, path + '/{0:06}'.format(item))
                    item += 1
        elif obj:
            self.emit_value(path, obj)

    def check_required(self, attr_list, container, container_name):
        """Verify that each name in attr_list is in the container"""
        for name in attr_list:
            if name not in container:
                raise ValueError("The '{0}' attribute is required in a {1}".
                                 format(name, container_name))

    def build_endpoints(self, config):
        """Generate an endpoint spec list from YAML config

        Builds a dictionary of endpoint definitions. The 'config' is a
        list of endpoint specifications. Each endpoint specification contains
        'name', 'host', and 'port' attributes. All attributes are
        expanded per the slurm hostlist rules. The length of the
        expanded name-list, must equal the length of the expanded
        host-list times the expanded port-list.

        Example:

        names : "nid[0001-0002]-[10001-10002]"
        hosts : "nid[0001-0002]"
        ports : "[10001-10002]"

        results in the following host-spec dictionary:

        {
        "nid0001-10001" : { "host" : "nid0001", "port" : 10001 },
        "nid0001-10002" : { "host" : "nid0001", "port" : 10002 },
        "nid0002-10001" : { "host" : "nid0002", "port" : 10001 },
        "nid0002-10002" : { "host" : "nid0002", "port" : 10002 }
        }

        """
        ep_dict = {}
        if 'endpoints' not in config:
            return ep_dict

        node_config = config['endpoints']
        for spec in node_config:
            self.check_required([ 'names', 'hosts', 'ports' ],
                                spec, "host specification")
            names = expand_names(spec['names'])
            hosts = expand_names(spec['hosts'])
            ports = expand_names(spec['ports'])
            if 'xprt' in spec:
                xprt = spec['xprt']
            else:
                xprt = 'sock'
            if 'auth' in spec:
                auth = spec['auth']
                self.check_required([ 'name' ],
                                    auth, "authentication configuration")
                auth_name = auth['name']
                if 'conf' in auth:
                    auth_config = auth['conf']
                else:
                    auth_config = ""
            else:
                auth_name = 'none'
                auth_config = ""
            # build compute for each host x port
            for host in hosts:
                for port in ports:
                    name = names.pop(0)
                    h = {
                        'name' : name,
                        'addr' : host,
                        'port' : port,
                        'xprt' : xprt,
                        'auth' : { 'name' : auth_name, 'conf' : auth_config }
                    }
                    ep_dict[name] = h

        return ep_dict

    def build_groups(self, config):
        groups = {}
        if 'groups' not in config:
            return groups
        group_conf = config['groups']
        for group_spec in group_conf:
            self.check_required(['name', 'endpoints', 'interfaces'],
                                group_spec, "daemon specification")
            endpoints = expand_names(group_spec['endpoints'])
            group_name = group_spec['name']
            interfaces = group_spec['interfaces']
            group = {
                'name'       : group_name,
                'endpoints'  : endpoints,
                'interfaces' : interfaces
            }
            groups[group_name] = group
        return groups

    def build_aggregators(self, config):
        aggregators = {}
        if 'aggregators' not in config:
            return aggregators

        agg_conf = config['aggregators']
        for agg_spec in agg_conf:
            self.check_required([ 'names', 'group', 'endpoints' ],
                                agg_spec, "aggregator specification")

            names = expand_names(agg_spec['names'])
            group = agg_spec['group']
            if group not in aggregators:
                aggregators[group] = []
            endpoints = expand_names(agg_spec['endpoints'])
            if len(names) != len(endpoints):
                raise ValueError('"aggregators:" The "host" and "name" specifications must '
                                'expand to the same number of names')
            for name in names:
                endpoint = endpoints.pop(0)
                agg = {
                        'name'      : name,
                        'endpoint'  : endpoint,
                        'state'     : 'stopped' # 'running', 'error'
                }
                aggregators[group].append(agg)
        return aggregators

    def build_producers(self, config):
        """
        Return a dictionary keyed by the group name. Each dictionary
        entry is a list of producers in that group.
        """
        producers = {}
        prod_spec = config['producers']
        for prod in prod_spec:
            self.check_required([ 'names', 'endpoints', 'updaters',
                                'reconnect', 'type', 'group' ],
                                prod, '"producer" entry')
            names = expand_names(prod['names'])
            endpoints = expand_names(prod['endpoints'])
            group = prod['group']
            if group not in producers:
                producers[group] = []

            if len(names) != len(endpoints):
                raise ValueError('"producer": The "host" and "name" specifications must '
                                'expand to the same number of strings')
            upd_spec = prod['updaters']

            # Expand and generate all the producers
            typ = prod['type']
            reconnect = prod['reconnect']
            for name in names:
                endpoint = endpoints.pop(0)
                prod = {
                    'name'      : name,
                    'endpoint'  : endpoint,
                    'type'      : typ,
                    'group'     : group,
                    'reconnect' : reconnect,
                    'updaters'  : upd_spec
                }
                producers[group].append(prod)
        return producers

    def build_updaters(self, config):
        """
        Return a dictionary keyed by the group name. Each dictionary
        entry is a list of updaters in that group.
        """
        updaters = {}
        for updtr_spec in config['updaters']:
            self.check_required([ 'name', 'group', 'interval', 'sets', 'producers' ],
                                updtr_spec, '"updater" entry')
            group = updtr_spec['group']
            if group not in updaters:
                updaters[group] = {}
            grp_updaters = updaters[group]
            updtr_name = updtr_spec['name']
            if updtr_name in grp_updaters:
                raise ValueError(f"Duplicate updater name '{updtr_name}''. "
                            "An updater name must be unique within the group")
            updtr = {
                'name'      : updtr_name,
                'interval'  : updtr_spec['interval'],
                'group'     : updtr_spec['group'],
                'sets'      : updtr_spec['sets'],
                'producers' : updtr_spec['producers']
            }
            if 'auto' in updtr_spec and 'push' in updtr_spec:
                raise ValueError(f"The updater specification: {json.dumps(updtr_spec)} "
                                "contains both 'push' and 'auto' which are "
                                "mutually exclusive")
            if 'auto' in updtr_spec:
                updtr['auto'] = updtr_spec['auto']
            if 'push' in updtr_spec:
                updtr['push'] = updtr_spec['push']

            grp_updaters[updtr_name] = updtr
        return updaters

    def build_stores(self, config):
        """
        Return a dictionary keyed by the group name. Each dictionary
        entry is a list of stores in that group.
        """
        stores = {}
        if 'stores' not in config:
            return None
        for store_spec in config['stores']:
            self.check_required([ 'name', 'group', 'plugin', 'container', 'schema' ],
                                store_spec, '"store" entry')
            group = store_spec['group']
            if group not in stores:
                stores[group] = {}
            grp_stores = stores[group]
            store_name = store_spec['name']
            if store_name in grp_stores:
                raise ValueError(f"Duplicate store name '{store_name}''. "
                            "A store name must be unique within the group")

            self.check_required([ 'name', 'config'],
                             store_spec['plugin'],
                            '"store plugin" entry')
            grp_stores[store_name] = store_spec
        return stores

    def build_samplers(self, config):
        """
        Generate samplers from YAML config.
        Return a dictionary keyed by the samplers group name. Each dictionary
        entry is a single ldms daemon's sampler configuration.
        """
        smplrs = {}
        for smplr_spec in config['samplers']:
            self.check_required([ 'names' ],
                                  smplr_spec, '"sampler" entry')
            smplrs[smplr_spec['names']] = smplr_spec
        return smplrs

    def build_plugins(self, config):
        """
        Generate plugins to load from a YAML config.
        Return a dictionary keyed by the plugin's group name. Each dictionary entry
        is a single plugin's configuration.
        """
        plugins = {}
        for plugn_spec in config['plugins']:
            self.check_required([ 'name', 'config' ],
                                  plugn_spec, '"plugin" entry')
            group = plugn_spec['group']
            if group not in plugins:
                plugins[group] = {}
            grp_plugins = plugins[group]
            plugin_name = plugn_spec['name']
            if plugin_name in grp_plugins:
                raise ValueError(f'Duplicate plugin name "{plugin_name}". '
                                  'Plugin must be unique within a group.')
            grp_plugins[plugin_name] = plugn_spec
        return plugins

    def __init__(self, client, name, cluster_config):
        """
        """
        self.client = client
        self.name = name
        self.cluster_config = cluster_config
        self.endpoints = self.build_endpoints(cluster_config)
        self.aggregators = self.build_aggregators(cluster_config)
        self.groups = self.build_groups(cluster_config)
        self.producers = self.build_producers(cluster_config)
        self.updaters = self.build_updaters(cluster_config)
        self.stores = self.build_stores(cluster_config)
        if 'samplers' in cluster_config:
            self.samplers = self.build_samplers(cluster_config)
        else:
            self.samplers = None
        if 'plugins' in cluster_config:
            self.plugins = self.build_plugins(cluster_config)
        else:
            self.plugins = None

    def cvt_time_spec_to_us(self, ts):
        if type(ts) == int:
            return ts * 1000000
        x = ts.find('us')
        if x > 0:
            return ts[:x]

        x = ts.find('ms')
        if x > 0:
            return int(float(ts[:x]) * 1000)

        x = ts.find('s')
        if x > 0:
            return int(float(ts[:x]) * 1000000)

        # assume seconds
        return int(float(ts) * 1000000)

    def commit(self):
        pass

    def save_config(self):
        try:
            self.client.delete_prefix('/' + self.name)
            self.walk(self.endpoints, '/' + self.name + '/endpoints')
            self.walk(self.aggregators, '/' + self.name + '/aggregators')
            self.walk(self.groups, '/' + self.name + '/groups')
            self.walk(self.producers, '/' + self.name + '/producers')
            self.walk(self.updaters, '/' + self.name + '/updaters')
            self.walk(self.stores, '/' + self.name + '/stores')
            if 'samplers' in self.cluster_config:
                self.walk(self.samplers, '/' + self.name + '/samplers')
            if 'plugins' in self.cluster_config:
                self.walk(self.plugins, '/' + self.name + '/plugins')
            self.client.put('/'+self.name+'/last_updated', str(time.time()))
        except Exception as e:
            a, b, c = sys.exc_info()
            print(str(e)+' '+str(c.tb_lineno))
            return 1

    def parse_to_cfg_str(self, cfg_obj):
        cfg_str = ''
        for key in cfg_obj:
            if key != 'interval':
                if len(cfg_str) > 8:
                    cfg_str += ' '
                cfg_str += key + '=' + str(cfg_obj[key])
        return cfg_str

    def config_v4(self, path):
        """
        Read the group configuration from ETCD and generate a version 4 LDMSD configuration
        This configuration assumes that the environemnt variables COMPONENT_ID, HOSTNAME
        all exist on the machines relevant to the ldmsd cluster.
        """
        for group_name in self.groups:
            # Load sampler statements
            group = self.groups[group_name]

            # Sampler config
            if self.samplers != None:
                try:
                    fd = open(f'{path}/{group_name}-samplers.conf', 'w+')
                    if self.plugins != None:
                        if group_name in self.plugins:
                            for plugin in self.plugins[group_name]:
                                cfg_str = self.parse_to_cfg_str(self.plugins[group_name][plugin]['config'])
                                fd.write(f'load name={plugin}\n')
                                fd.write(f'config name={plugin} {cfg_str}\n\n')

                    for smplr_group in self.samplers:
                        # TO DO: Refactor sampler config architecture to more easily reference appropriate groups
                        if group_name != self.samplers[smplr_group]['group']:
                            continue
                        for sampler in self.samplers[smplr_group]['config']:
                            cfg_str = self.parse_to_cfg_str(sampler)
                            sname = sampler['name']
                            interval = cvt_intrvl_str_to_us(sampler['interval'].split(':')[0])
                            offset = cvt_intrvl_str_to_us(sampler['interval'].split(':')[1])
                            fd.write(f'load name={sname}\n')
                            fd.write(f'config {cfg_str} producer=${{HOSTNAME}} '+
                                     f'component_id=${{COMPONENT_ID}} '+
                                     f'instance=${{HOSTNAME}}/{sname}\n')
                            fd.write(f'start name={sname} interval={interval} offset={offset}\n')
                    fd.close()
                except Exception as e:
                    a, b, d = sys.exc_info()
                    print(f'Error generating sampler configuration: {str(e)} {str(d.tb_lineno)}')
                    return errno.EINVAL
            else:
                print(f'"samplers" not found in configuration file. Skipping...')

            # Agg config
            try:
                ''' "Balance" agg configuration if all samplers are included in each aggregator '''
                if group_name not in self.aggregators:
                    continue
                last_sampler = None
                for agg in self.aggregators[group_name]:
                    fd = open(f'{path}/{group_name}-{agg["name"]}.conf', 'w+')
                    if group_name in self.producers:
                        ''' Balance samplers across aggregators '''
                        prdcrs = []
                        loop = round(len(self.producers[group_name]) / len(self.aggregators[group_name]))
                        for prdcr in self.producers[group_name]:
                            prdcrs.append(prdcr["name"])
                        if not last_sampler:
                            idx = 0
                        else:
                            idx = prdcrs.index(last_sampler) + 1
                        prod_group = self.producers[group_name][idx:]
                        i = 0
                        for producer in prod_group:
                            if i >= loop:
                                break
                            regex = False
                            pname = producer['name']
                            port = self.endpoints[producer['endpoint']]['port']
                            xprt = self.endpoints[producer['endpoint']]['xprt']
                            hostname = self.endpoints[producer['endpoint']]['addr']
                            ptype = producer['type']
                            interval = cvt_intrvl_str_to_us(producer['reconnect'])
                            fd.write(f'prdcr_add name={pname} '+
                                     f'host={hostname} '+
                                     f'port={port} '+
                                     f'xprt={xprt} type={ptype} interval={interval}\n')
                            last_sampler = pname
                            i += 1
                            if 'regex' in producer:
                                regex = True
                                fd.write(f'prdcr_start_regex regex={producer["regex"]}\n')
                        if not regex:
                            fd.write('prdcr_start_regex regex=.*\n')
                    # Plugin Config
                    if self.plugins != None:
                        if group_name in self.plugins:
                            for plugin in self.plugins[group_name]:
                                cfg_str = self.parse_to_cfg_str(self.plugins[group_name][plugin]['config'])
                                if 'stream' in self.plugins[group_name][plugin]['config']:
                                    # Will need to be updated for custom producer regex
                                    fd.write(f'prdcr_subscribe stream={self.plugins[group_name][plugin]["config"]["stream"]} regex=.*\n')
                                fd.write(f'load name={plugin}\n')
                                fd.write(f'config name={plugin} {cfg_str}\n\n')

                    if group_name in self.updaters:
                        updtr_group = self.updaters[group_name]
                        for updtr in updtr_group:
                            interval = cvt_intrvl_str_to_us(updtr_group[updtr]['interval'].split(':')[0])
                            offset = cvt_intrvl_str_to_us(updtr_group[updtr]['interval'].split(':')[1])
                            fd.write(f'updtr_add name={updtr_group[updtr]["name"]} '+
                                     f'interval={interval} offset={offset} auto_interval=true\n')
                            for prod in updtr_group[updtr]['producers']:
                                fd.write(f'updtr_prdcr_add name={updtr_group[updtr]["name"]} '+
                                         f'regex={prod["regex"]}\n')
                            fd.write(f'updtr_start name={updtr_group[updtr]["name"]}\n')
                    if group_name in self.stores:
                        store_group = self.stores[group_name]
                        loaded_plugins = []
                        for store in store_group:
                            if store_group[store]["plugin"]["name"] not in loaded_plugins:
                                fd.write(f'load name={store_group[store]["plugin"]["name"]}\n')
                                fd.write(f'config name={store_group[store]["plugin"]["name"]} '+
                                         f'{self.parse_to_cfg_str(store_group[store]["plugin"]["config"])}\n')
                                loaded_plugins.append(store_group[store]["plugin"]["name"])
                            fd.write(f'strgp_add name={store} plugin={store_group[store]["plugin"]["name"]} '+
                                     f'container={store_group[store]["container"]} '+
                                     f'schema={store_group[store]["schema"]} '+
                                     f'flush={store_group[store]["flush"]}\n')
                            fd.write(f'strgp_start name={store}\n')
            except Exception as e:
                ea, eb, ec = sys.exc_info()
                print('Agg config Error: '+str(e)+' Line:'+str(ec.tb_lineno))

    def config_v5(self):
        pass

    def config(self, version=4):
        if version == 4:
            return self.config_v4()
        elif version == 5:
            return self.config_v5()
        raise ValueError("Version {0} is not recognized".format(version))

def expand_names(name_spec):
    if type(name_spec) != str and isinstance(name_spec, collections.Sequence):
        names = []
        for name in name_spec:
            names += hostlist.expand_hostlist(name)
    else:
        names = hostlist.expand_hostlist(name_spec)
    return names

def cvt_intrvl_str_to_us(interval_s):
    """Converts a time interval string to microseconds

    A time-interval string is an integer or float follows by a
    unit-string. A unit-string is any of the following:

    's'  - seconds
    'us' - microseconds
    'm'  - minutes

    Unit strings are not case-sensitive.

    Examples:
    '1.5s' - 1.5 seconds
    '1.5S' - 1.5 seconds
    '2s'   - 2 seconds
    """
    interval_s = interval_s.lower()
    if 'us' in interval_s:
        factor = 1
        ival_s = interval_s.replace('us','')
    if 'ms' in interval_s:
        factor = 1000
        ival_s = interval_s.replace('ms','')
    elif 's' in interval_s:
        factor = 1000000
        ival_s = interval_s.replace('s','')
    elif 'm' in interval_s:
        factor = 60000000
        ival_s = interval_s.replace('m','')
    try:
        mult = float(ival_s)
    except:
        raise ValueError(f"{interval_s} is not a valid time-interval string")
    return int(mult * factor)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="LDMS Monitoring Cluster Configuration")
    parser.add_argument("--cluster", metavar="FILE", required=True,
                        help="The name of the etcd cluster configuration file")
    parser.add_argument("--ldms_config", metavar="FILE", required=True,
                        help="The ldmsd load balance domain configuration file. "
                        "This will not start the maestro "
                        "load balancer")
    parser.add_argument("--prefix", metavar="STRING", required=True,
                        help="The prefix for the dumped aggregator configurations",
                        default="unknown")
    parser.add_argument("--generate-config-path", metavar="STRING", required=False,
                        default=False)
    parser.add_argument("--etcd", action=MaestroBooleanOptionalAction,
                        help="Enable transmission to etcd (default: no)");
    parser.add_argument("--debug", action=MaestroBooleanOptionalAction);
    parser.add_argument("--version", metavar="VERSION",
                        help="The OVIS version for the output syntax (4 or 5), default is 4",
                        default=4)
    args = parser.parse_args()

    # Load the cluster configuration file. This configures the daemons
    # that support the key/value configuration database
    etcd_fp = open(args.cluster)
    etcd_spec = yaml.safe_load(etcd_fp)

    pfx = etcd_spec['cluster']
    if args.etcd:
        etcd_hosts = ()
        for h in etcd_spec['members']:
            etcd_hosts += (( h['host'], h['port'] ),)

    # All keys in the DB are prefixed with the cluster name, 'pfx'. So we can
    # have multiple monitoring hosted by the same consensus cluster.
    config_fp = open(args.ldms_config)
    conf_spec = yaml.safe_load(config_fp)

    # Use the 1st host for now
    if args.etcd:
        import etcd3
        client = etcd3.client(host=etcd_hosts[0][0], port=etcd_hosts[0][1],
        grpc_options=[ ('grpc.max_send_message_length',16*1024*1024),
                       ('grpc.max_receive_message_length',16*1024*1024)])
    else:
        client = NoetcdClient()

    cluster = Cluster(client, args.prefix, conf_spec)
    if args.generate_config_path:
        cluster.config_v4(args.generate_config_path)
        print("LDMSD v4 config files generated")
        sys.exit(0)

    # blow away the existing configuration
    rc = cluster.save_config()
    if rc:
        print("Error saving ldms cluster configuration to etcd cluster")
        sys.exit(0)
    print("LDMS aggregator configuration saved to etcd cluster.")
    if not args.etcd and args.debug:
        client.print_all()

    sys.exit(0)
