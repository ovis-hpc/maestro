#!/usr/bin/env python3
import os, sys
import errno
import yaml
import hostlist
import collections
import argparse
import etcd3
import json
import time
from collections import Mapping, Sequence

class Cluster(object):
    def emit_value(self, path, value):
        try:
            res = client.put(path, str(value))
        except Exception as e:
            print("Error {0} setting {1} : {2}".format(str(e), path, str(value)))

    def walk(self, obj, path=''):
        if isinstance(obj, Mapping):
            for key in obj:
                self.walk(obj[key], '{0}/{1}'.format(path, str(key)))
        elif isinstance(obj, Sequence):
            if isinstance(obj, (str, bytearray)):
                self.emit_value(path, obj)
            else:
                item = 0
                for v in obj:
                    # we want keys to be returned in numerical order which requires z-fill
                    self.walk(v, path + '/{0:06}'.format(item))
                    item += 1
        elif obj:
            self.emit_value(path, obj)

    def check_required(self, attr_list, container, container_name):
        """Verify that each name in attr_list is in the container"""
        for name in attr_list:
            if name not in container:
                raise ValueError("The '{0}' attribute is required in a {1}".
                                 format(name, container_name))

    def build_endpoints(self, config):
        """Generate an endpoint spec list from YAML config

        Builds a dictionary of endpoint definitions. The 'config' is a
        list of endpoint specifications. Each endpoint specification contains
        'name', 'host', and 'port' attributes. All attributes are
        expanded per the slurm hostlist rules. The length of the
        expanded name-list, must equal the length of the expanded
        host-list times the expanded port-list.

        Example:

        names : "nid[0001-0002]-[10001-10002]"
        hosts : "nid[0001-0002]"
        ports : "[10001-10002]"

        results in the following host-spec dictionary:

        {
        "nid0001-10001" : { "host" : "nid0001", "port" : 10001 },
        "nid0001-10002" : { "host" : "nid0001", "port" : 10002 },
        "nid0002-10001" : { "host" : "nid0002", "port" : 10001 },
        "nid0002-10002" : { "host" : "nid0002", "port" : 10002 }
        }

        """
        ep_dict = {}
        if 'endpoints' not in config:
            return ep_dict

        node_config = config['endpoints']
        for spec in node_config:
            self.check_required([ 'names', 'hosts', 'ports' ],
                                spec, "host specification")
            names = expand_names(spec['names'])
            hosts = expand_names(spec['hosts'])
            ports = expand_names(spec['ports'])
            if 'xprt' in spec:
                xprt = spec['xprt']
            else:
                xprt = 'sock'
            if 'auth' in spec:
                auth = spec['auth']
                self.check_required([ 'name' ],
                                    auth, "authentication configuration")
                auth_name = auth['name']
                if 'config' in auth:
                    auth_config = auth['config']
                else:
                    auth_config = ""
            else:
                auth_name = 'none'
                auth_config = ""
            # build compute for each host x port
            for host in hosts:
                for port in ports:
                    name = names.pop(0)
                    h = {
                        'name' : name,
                        'addr' : host,
                        'port' : port,
                        'xprt' : xprt,
                        'auth' : { 'name' : auth_name, 'config' : auth_config }
                    }
                    ep_dict[name] = h

        return ep_dict

    def build_groups(self, config):
        groups = {}
        if 'groups' not in config:
            return groups
        group_conf = config['groups']
        for group_spec in group_conf:
            self.check_required(['name', 'endpoints', 'interfaces'],
                                group_spec, "daemon specification")
            endpoints = expand_names(group_spec['endpoints'])
            group_name = group_spec['name']
            interfaces = group_spec['interfaces']
            group = {
                'name'       : group_name,
                'endpoints'  : endpoints,
                'interfaces' : interfaces
            }
            groups[group_name] = group
        return groups

    def build_aggregators(self, config):
        aggregators = {}
        if 'aggregators' not in config:
            return aggregators

        agg_conf = config['aggregators']
        for agg_spec in agg_conf:
            self.check_required([ 'names', 'group', 'endpoints' ],
                                agg_spec, "aggregator specification")

            names = expand_names(agg_spec['names'])
            group = agg_spec['group']
            if group not in aggregators:
                aggregators[group] = []
            endpoints = expand_names(agg_spec['endpoints'])
            if len(names) != len(endpoints):
                raise ValueError('"aggregators:" The "host" and "name" specifications must '
                                'expand to the same number of names')
            for name in names:
                endpoint = endpoints.pop(0)
                agg = {
                        'name'      : name,
                        'endpoint'  : endpoint,
                        'state'     : 'stopped' # 'running', 'error'
                }
                aggregators[group].append(agg)
        return aggregators

    def build_producers(self, config):
        """
        Return a dictionary keyed by the group name. Each dictionary
        entry is a list of producers in that group.
        """
        producers = {}
        prod_spec = config['producers']
        for prod in prod_spec:
            self.check_required([ 'names', 'endpoints', 'updaters',
                                'reconnect', 'type', 'group' ],
                                prod, '"producer" entry')
            names = expand_names(prod['names'])
            endpoints = expand_names(prod['endpoints'])
            group = prod['group']
            if group not in producers:
                producers[group] = []

            if len(names) != len(endpoints):
                raise ValueError('"producer": The "host" and "name" specifications must '
                                'expand to the same number of strings')
            upd_spec = prod['updaters']

            # Expand and generate all the producers
            typ = prod['type']
            reconnect = prod['reconnect']
            for name in names:
                endpoint = endpoints.pop(0)
                prod = {
                    'name'      : name,
                    'endpoint'  : endpoint,
                    'type'      : typ,
                    'group'     : group,
                    'reconnect' : reconnect,
                    'updaters'  : upd_spec
                }
                producers[group].append(prod)
        return producers

    def build_updaters(self, config):
        """
        Return a dictionary keyed by the group name. Each dictionary
        entry is a list of updaters in that group.
        """
        updaters = {}
        for updtr_spec in config['updaters']:
            self.check_required([ 'name', 'group', 'interval', 'sets', 'producers' ],
                                updtr_spec, '"updater" entry')
            group = updtr_spec['group']
            if group not in updaters:
                updaters[group] = {}
            grp_updaters = updaters[group]
            updtr_name = updtr_spec['name']
            if updtr_name in grp_updaters:
                raise ValueError(f"Duplicate updater name '{updtr_name}''. "
                            "An updater name must be unique within the group")
            updtr = {
                'name'      : updtr_name,
                'interval'  : updtr_spec['interval'],
                'group'     : updtr_spec['group'],
                'sets'      : updtr_spec['sets'],
                'producers' : updtr_spec['producers']
            }
            if 'auto' in updtr_spec and 'push' in updtr_spec:
                raise ValueError(f"The updater specification: {json.dumps(updtr_spec)} "
                                "contains both 'push' and 'auto' which are "
                                "mutually exclusive")
            if 'auto' in updtr_spec:
                updtr['auto'] = updtr_spec['auto']
            if 'push' in updtr_spec:
                updtr['push'] = updtr_spec['push']

            grp_updaters[updtr_name] = updtr
        return updaters

    def build_stores(self, config):
        """
        Return a dictionary keyed by the group name. Each dictionary
        entry is a list of stores in that group.
        """
        stores = {}
        if 'stores' not in config:
            return None
        for store_spec in config['stores']:
            self.check_required([ 'name', 'group', 'plugin', 'container', 'schema' ],
                                store_spec, '"store" entry')
            group = store_spec['group']
            if group not in stores:
                stores[group] = {}
            grp_stores = stores[group]
            store_name = store_spec['name']
            if store_name in grp_stores:
                raise ValueError(f"Duplicate store name '{store_name}''. "
                            "A store name must be unique within the group")

            self.check_required([ 'name', 'config'],
                             store_spec['plugin'],
                            '"store plugin" entry')
            grp_stores[store_name] = store_spec
        return stores

    def build_samplers(self, config):
        """
        Generate samplers from YAML config.
        Return a dictionary keyed by the samplers group name. Each dictionary
        entry is a single ldms daemon's sampler configuration.
        """
        smplrs = {}
        for smplr_spec in config['samplers']:
            self.check_required([ 'names' ],
                                  smplr_spec, '"sampler" entry')
            smplrs[smplr_spec['names']] = smplr_spec
        return smplrs

    def __init__(self, client, name, cluster_config):
        """
        """
        self.client = client
        self.name = name
        self.endpoints = self.build_endpoints(cluster_config)
        self.aggregators = self.build_aggregators(cluster_config)
        self.groups = self.build_groups(cluster_config)
        self.producers = self.build_producers(cluster_config)
        self.updaters = self.build_updaters(cluster_config)
        self.stores = self.build_stores(cluster_config)
        self.samplers = self.build_samplers(cluster_config)

    def cvt_time_spec_to_us(self, ts):
        if type(ts) == int:
            return ts * 1000000
        x = ts.find('us')
        if x > 0:
            return ts[:x]

        x = ts.find('ms')
        if x > 0:
            return int(float(ts[:x]) * 1000)

        x = ts.find('s')
        if x > 0:
            return int(float(ts[:x]) * 1000000)

        # assume seconds
        return int(float(ts) * 1000000)

    def commit(self):
        pass

    def save_config(self):
        self.client.delete_prefix('/' + self.name)
        self.walk(self.endpoints, '/' + self.name + '/endpoints')
        self.walk(self.aggregators, '/' + self.name + '/aggregators')
        self.walk(self.groups, '/' + self.name + '/groups')
        self.walk(self.samplers, '/' + self.name + '/samplers')
        self.walk(self.producers, '/' + self.name + '/producers')
        self.walk(self.updaters, '/' + self.name + '/updaters')
        self.walk(self.stores, '/' + self.name + '/stores')
        self.client.put('/'+self.name+'/last_updated', str(time.time()))

    def parse_to_cfg_str(self, cfg_obj):
        cfg_str = 'config '
        for key in cfg_obj:
            if key != 'interval':
                if len(cfg_str) > 8:
                    cfg_str += ' '
                cfg_str += key + '=' + str(cfg_obj[key])
        return cfg_str

    def config_v4(self, path):
        """
        Read the group configuration from ETCD and generate a version 4 LDMSD configuration
        This configuration assumes that the environemnt variables COMPONENT_ID, HOSTNAME
        all exist on the machines relevant to the ldmsd cluster.
        """
        for group_name in self.groups:
            fd = open(f'{path}/{group_name}.conf', 'w+')
            # Load sampler statements
            group = self.groups[group_name]
            # Sampler config
            try:
                for smplr_group in self.samplers:
                    # TO DO: Refactor sampler config architecture to more easily reference appropriate groups
                    if group_name != self.samplers[smplr_group]['group']:
                        continue
                    for sampler in self.samplers[smplr_group]['config']:
                        cfg_str = self.parse_to_cfg_str(sampler)
                        sname = sampler['name']
                        interval = cvt_intrvl_str_to_us(sampler['interval'].split(':')[0])
                        offset = cvt_intrvl_str_to_us(sampler['interval'].split(':')[1])
                        fd.write(f'load name={sname}\n')
                        fd.write(f'{cfg_str} producer=${{HOSTNAME}} '+
                                 f'component_id=${{COMPONENT_ID}} '+
                                 f'instance=${{HOSTNAME}}/{sname}\n')
                        fd.write(f'start name={sname} interval={interval} offset={offset}\n')
            except Exception as e:
                a, b, d = sys.exc_info()
                print(str(e) + str(d.tb_lineno))
                return errno.EINVAL

            # Agg config
            try:
                if group_name in self.producers:
                    prod_group = self.producers[group_name]
                    for producer in prod_group:
                        pname = producer['name']
                        port = self.endpoints[producer['endpoint']]['port']
                        xprt = self.endpoints[producer['endpoint']]['xprt']
                        hostname = self.endpoints[producer['endpoint']]['addr']
                        ptype = producer['type']
                        interval = cvt_intrvl_str_to_us(producer['reconnect'])
                        fd.write(f'prdcr_add name={pname} '+
                                 f'host={hostname} '+
                                 f'port={port} '+
                                 f'xprt={xprt} type={ptype} interval={interval}\n')
                    fd.write('prdcr_start_regex regex=.*\n')
                if group_name in self.updaters:
                    updtr_group = self.updaters[group_name]
                    for updtr in updtr_group:
                        interval = cvt_intrvl_str_to_us(updtr_group[updtr]['interval'].split(':')[0])
                        offset = cvt_intrvl_str_to_us(updtr_group[updtr]['interval'].split(':')[1])
                        fd.write(f'updtr_add name={updtr_group[updtr]["name"]} '+
                                 f'interval={interval} offset={offset} auto_interval=true\n')
                        for prod in updtr_group[updtr]['producers']:
                            fd.write(f'updtr_prdcr_add name={updtr_group[updtr]["name"]} '+
                                     f'regex={prod["regex"]}\n')
                        fd.write(f'updtr_start name={updtr_group[updtr]["name"]}\n')
                if group_name in self.stores:
                    store_group = self.stores[group_name]
                    for store in store_group:
                        fd.write(f'load name={store_group[store]["plugin"]["name"]}\n')
                        fd.write(f'{self.parse_to_cfg_str(store_group[store]["plugin"]["config"])}\n')
                        fd.write(f'strgp_add name={store} plugin={store_group[store]["plugin"]["name"]} '+
                                 f'container={store_group[store]["container"]} '+
                                 f'schema={store_group[store]["schema"]} '+
                                 f'flush=60s\n')
                        fd.write(f'strgp_start name={store}\n')
            except Exception as e:
                ea, eb, ec = sys.exc_info()
                print(str(e)+str(ec.tb_lineno))

    def config_v5(self):
        pass

    def config(self, version=4):
        if version == 4:
            return self.config_v4()
        elif version == 5:
            return self.config_v5()
        raise ValueError("Version {0} is not recognized".format(version))

def expand_names(name_spec):
    if type(name_spec) != str and isinstance(name_spec, collections.Sequence):
        names = []
        for name in name_spec:
            names += hostlist.expand_hostlist(name)
    else:
        names = hostlist.expand_hostlist(name_spec)
    return names

def cvt_intrvl_str_to_us(interval_s):
    """Converts a time interval string to microseconds

    A time-interval string is an integer or float follows by a
    unit-string. A unit-string is any of the following:

    's'  - seconds
    'us' - microseconds
    'm'  - minutes

    Unit strings are not case-sensitive.

    Examples:
    '1.5s' - 1.5 seconds
    '1.5S' - 1.5 seconds
    '2s'   - 2 seconds
    """
    interval_s = interval_s.lower()
    if 'us' in interval_s:
        factor = 1
        ival_s = interval_s.replace('us','')
    if 'ms' in interval_s:
        factor = 1000
        ival_s = interval_s.replace('ms','')
    elif 's' in interval_s:
        factor = 1000000
        ival_s = interval_s.replace('s','')
    elif 'm' in interval_s:
        factor = 60000000
        ival_s = interval_s.replace('m','')
    try:
        mult = float(ival_s)
    except:
        raise ValueError(f"{interval_s} is not a valid time-interval string")
    return int(mult * factor)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="LDMS Monitoring Cluster Configuration")
    parser.add_argument("--cluster", metavar="FILE", required=True,
                        help="The name of the etcd cluster configuration file")
    parser.add_argument("--ldms_config", metavar="FILE", required=True,
                        help="The ldmsd load balance domain configuration file. "
                        "This will not start the maestro "
                        "load balancer")
    parser.add_argument("--prefix", metavar="STRING", required=True,
                        help="The prefix for the dumped aggregator configurations",
                        default="unknown")
    parser.add_argument("--generate-config-path", metavar="STRING", required=False,
                        default=False)
    parser.add_argument("--version", metavar="VERSION",
                        help="The OVIS version for the output syntax (4 or 5), default is 4",
                        default=4)
    args = parser.parse_args()

    # Load the cluster configuration file. This configures the daemons
    # that support the key/value configuration database
    etcd_fp = open(args.cluster)
    etcd_spec = yaml.safe_load(etcd_fp)

    pfx = etcd_spec['cluster']
    etcd_hosts = ()
    for h in etcd_spec['members']:
        etcd_hosts += (( h['host'], h['port'] ),)

    # All keys in the DB are prefixed with the cluster name, 'pfx'. So we can
    # have multiple monitoring hosted by the same consensus cluster.
    config_fp = open(args.ldms_config)
    conf_spec = yaml.safe_load(config_fp)

    # Use the 1st host for now
    client = etcd3.client(host=etcd_hosts[0][0], port=etcd_hosts[0][1],
        grpc_options=[ ('grpc.max_send_message_length',16*1024*1024),
                       ('grpc.max_receive_message_length',16*1024*1024)])

    cluster = Cluster(client, args.prefix, conf_spec)
    if args.generate_config_path:
        cluster.config_v4(args.generate_config_path)
        print("LDMSD v4 config files generated")
        sys.exit(0)

    # blow away the existing configuration
    cluster.save_config()
    print("LDMS aggregator configuration saved to etcd cluster.")

    sys.exit(0)
