#!/usr/bin/env python3
import os, sys
import subprocess
import errno
import yaml
import argparse
import etcd3
import json
import threading, queue
import time
from Communicator import Communicator
from maestro_util import cvt_intrvl_str_to_us, expand_names

def agg_hosts(grp_name, aggs, daemons):
    return [ daemons[grp_name][d['endpoint']] for d in aggs[grp_name] ]

class MaestroMonitor(object):
    def __init__(self, prefix, config, args):
        self.prefix = args.prefix
        if self.prefix[0] == '/':
            self.prefix = prefix[1:]
        # Use the 1st etcd host for now
        self.client = etcd3.client(host=etcd_hosts[0][0], port=etcd_hosts[0][1],
                                   grpc_options=[ ('grpc.max_send_message_length',16*1024*1024),
                                                  ('grpc.max_receive_message_length',16*1024*1024)])
        self.config = self.load_config()
        self.args = args
        self.json_config = json.dumps(config)
        self.do_rebalance = False
        self.res_samplers = False
        self.reset_conf()
        self.comms = self.build_comms()
        self.etcd_event_handler = {
            'endpoints'   : self.__reconf_endpoints,
            'daemons'     : self.__reconf_daemons,
            'aggregators' : self.__reconf_aggregators,
            'producers'   : self.__reconf_producers,
            'samplers'    : self.__reconf_samplers,
            'stores'      : self.__reconf_stores,
            'updaters'    : self.__reconf_updaters
        }

    def reset_conf(self):
        self.daemons = self.config[self.prefix]['daemons']
        if 'samplers' in self.config[self.prefix]:
            self.samplers = self.config[self.prefix]['samplers']
        else:
            self.samplers = {}
        self.producers = self.config[self.prefix]['producers']
        self.aggregators = self.config[self.prefix]['aggregators']
        self.updaters = self.config[self.prefix]['updaters']

    def build_comms(self):
        comms = {}
        for grp_name in self.aggregators:
            comms[grp_name] = {}
            for agg_host in agg_hosts(grp_name, self.aggregators, self.daemons):
                if self.args.start_aggregators:
                    # For now create log and pid files in log subdirectory
                    subprocess.run(['ldmsd',
                                    '-x', agg_host['xprt']+':'+agg_host['port'],
                                    '-a', agg_host['auth']['name'],
                                    '-l', 'log/'+agg_host['name']+'.log',
                                    '-m', '2g',
                                    '-r', 'log/'+agg_host['name']+'.pid'])
                if 'conf' in agg_host['auth']:
                    conf = agg_host['auth']['conf']
                else:
                    conf = None
                comm = Communicator(agg_host['xprt'],
                                    agg_host['addr'],
                                    agg_host['port'],
                                    agg_host['auth']['name'],
                                    { 'conf' : conf })
                comms[grp_name][agg_host['name']] = comm
                comm.connect()
        for daemon_grp in self.samplers:
            comms[daemon_grp] = {}
            for ep_name in self.daemons[daemon_grp]:
                ep = self.daemons[daemon_grp][ep_name]
                if ep['maestro_comm'] is False:
                    continue
                if 'conf' in ep['auth']:
                    conf = ep['auth']['conf']
                else:
                    conf = None
                comm = Communicator(ep['xprt'],
                                    ep['addr'],
                                    ep['port'],
                                    ep['auth']['name'],
                                    {'conf':conf})
                comms[daemon_grp][ep_name] = comm
                comm.connect()
        return comms

    def check_samplers(self):
        cfg_smplrs = {}
        for dmn_grp in self.samplers:
            cfg_smplrs[dmn_grp] = {}
            for sampler in self.comms[dmn_grp]:
                if self.comms[dmn_grp][sampler].state == 3:
                    print(f'Unable to connect to ldms daemon {sampler}')
                    continue
                rc, msg = self.comms[dmn_grp][sampler].smplr_status()
                if rc:
                    print(f'Error {rc}: {msg} getting {sampler} status\n')
                if msg is not None and len(msg) < len(self.samplers[dmn_grp]['plugins']):
                    cfg_smplrs[dmn_grp][sampler] = self.daemons[dmn_grp][sampler]
                elif len(msg) == len(self.samplers[dmn_grp]['plugins']):
                    for plugin in self.samplers[dmn_grp]['plugins']:
                        print(f'Sampler {plugin["name"]} has already been started on {sampler}')
        return cfg_smplrs

    def load_config(self):
        """
        Build a configuration dictionary from the k,v
        in the ETCD store
        """
        pfx = self.prefix
        if pfx[0] != '/':
            pfx = '/' + self.prefix
        config = {}
        kv_config = self.client.get_prefix(pfx)
        for v, m in kv_config:
            path = m.key.decode('utf-8')
            s = path.split('/')
            if len(s[0]) == 0:
                # if path begins with '/' strip 1st value
                s = s[1:]
            cfg = config
            for i in range(0, len(s[:-1])):
                d = s[i]
                if d.isdigit():
                    # This is a list entry, append a new dictionary if necessary
                    if len(cfg) <= int(d):
                        cfg.append({})
                    cfg = cfg[int(d)]
                else:
                    if d not in cfg:
                        # look ahead to see if we are adding a dictionary or a list
                        if s[i+1].isdigit():
                            cfg[d] = []
                        else:
                            cfg[d] = {}
                    cfg = cfg[d]
            if isinstance(cfg, list):
                cfg.append(v.decode('utf-8'))
            else:
                cfg[s[-1]] = v.decode('utf-8')
        return config

    def __reconf_endpoints(self, config):
        pass

    def __reconf_daemons(self, config):
        pass

    def __reconf_aggregators(self, config):
        pass

    def __reconf_producers(self, config):
        pass

    def __reconf_samplers(self):
        self.res_samplers = True

    def __reconf_stores(self, config):
        pass

    def __reconf_updaters(self, config):
        pass

    def rebalance(self, agg_state):
        """
        Re-build the config for each of the load balance groups
        """
        groups = {}
        if 'daemons' in self.config[self.prefix]:
            self.daemons = self.config[self.prefix]['daemons']
        else:
            self.daemons = {}
        if 'producers' in self.config[self.prefix]:
            self.producers = self.config[self.prefix]['producers']
        else:
            self.producers = {}
        if 'samplers' in self.config[self.prefix]:
            self.samplers = self.config[self.prefix]['samplers']
        else:
            self.samplers = {}
        if 'aggregators' in self.config[self.prefix]:
            self.aggregators = self.config[self.prefix]['aggregators']
        else:
            self.aggregators = {}
        if 'stores' in self.config[self.prefix]:
            self.stores = self.config[self.prefix]['stores']
        else:
            self.stores = {}
        for group in self.producers:
            grp_prods = list(self.producers[group].keys())
            prod_count = len(grp_prods)
            grp_aggs = []
            for agg in self.aggregators[group]:
                if agg_state[agg['endpoint']] != 'ready':
                    continue
                grp_aggs.append(agg)
            agg_count = len(grp_aggs)
            if agg_count != 0:
                prods_per_agg = prod_count // agg_count
                remainder = prod_count % agg_count
            else:
                # None of the aggregators are running
                break
            # Assign producers to aggregators
            prod_idx = 0
            groups[group] = []
            for agg in grp_aggs:
                agg_prods = []
                for cnt in range(0, prods_per_agg):
                    agg_prods.append(grp_prods[prod_idx])
                    prod_idx += 1
                if remainder > 0:
                    agg_prods.append(grp_prods[prod_idx])
                    prod_idx += 1
                    remainder -= 1
                groups[group].append({ "aggregator" : agg, "producers" : agg_prods })

        # Add all the producers to the aggregators. Any error here is not an issue
        # because all aggs have all producers, although, not all producers are started.
        add_producers(self.comms, self.producers, self.aggregators, self.daemons, agg_state)

        # Add all the updaters and storage policies
        add_updaters(self.comms, self.updaters, self.aggregators, self.daemons, agg_state)
        add_stores(self.comms, self.stores, self.aggregators, self.daemons, agg_state)

        cfg_sampler_eps = self.check_samplers()
        if not self.res_samplers:
            if cfg_sampler_eps:
                config_samplers(self.comms, self.samplers, cfg_sampler_eps)
                start_samplers(self.comms, self.samplers, cfg_sampler_eps)
        else:
            if cfg_sampler_eps:
                config_samplers(self.comms, self.samplers, cfg_sampler_eps)
            restart_samplers(self.comms, self.samplers, self.daemons)
            self.res_samplers = False
        # Start the producers assigned to each of the Aggregators
        start_producers(self.comms, groups)
        return groups

    def __etcd_caller(self, event):
        thread = threading.Thread(target=self.__etcd_callback, args=(event.events[0],))
        thread.start()
        thread.join()

    def __etcd_callback(self, event):
        try:
            self.lock.acquire()
            if type(event) != etcd3.events.PutEvent:
                self.lock.release()
                return
            print('Change in etcd cluster configuration.\nReconfiguring...')
            config = self.load_config()
            for conf in self.config[self.prefix]:
                if conf == 'last_updated':
                    continue
                if config[self.prefix][conf] != self.config[self.prefix][conf]:
                    self.etcd_event_handler[conf]()
                    self.do_rebalance = True
                    break
            self.config = config
            self.lock.release()
        except Exception as e:
            a, b, c = sys.exc_info()
            print(str(e) + ' ' + str(c.tb_lineno)+'\n')

    def cluster_monitor(self):
        last_state = {}
        pfx = '/'+self.prefix
        self.lock = threading.Lock()
        self.chng_id = self.client.add_watch_callback(pfx+'/last_updated',
                                                      self.__etcd_caller)
        while True:
            ldmsd_state = query_ldmsd_state(self.client, self.comms)
            for dmn in ldmsd_state:
                if dmn not in last_state:
                    self.do_rebalance = True
                elif last_state[dmn] != ldmsd_state[dmn]:
                    self.do_rebalance = True
                last_state[dmn] = ldmsd_state[dmn]
            if self.do_rebalance:
                print('Rebalance cluster...')
                rc = self.rebalance(ldmsd_state)
                self.do_rebalance = False
                print('Finished load balancing.')
            time.sleep(1)

def config_samplers(comms, samplers, daemons):
    """Configure and load plugins for sampler daemons"""
    for smplr_group in samplers:
        for smplr in daemons[smplr_group]:
            smplr_host = daemons[smplr_group][smplr]
            smplr_name = smplr_host['name']
            # Establish communicator to each ldmsd sampler
            comm = comms[samplers[smplr_group]['daemons']][smplr_host['name']]
            err, res = comm.daemon_status()
            if err:
                print(f"Lost connectivity to {smplr_host['name']} err {err}")
                return False
            print(f"Adding sampler plugins to sampler {smplr_name}")
            for plugin in samplers[smplr_group]['plugins']:
                err, rc = comm.plugn_load(plugin['name'])
                if err == 17:
                    print(f"File exists: {rc}")
                elif err != 0:
                    print(f"Error {err} loading plugin {plugin['name']}")
                    continue
                # Auto set producer_name/instance_name first,
                # then overwrite if specfied in config
                cfg_args = { "producer"  : smplr_host['name'],
                             "instance"  : smplr_host['name'] +'/'+plugin['name'] }
                for attr in plugin['config']:
                    if attr == 'name' or attr == 'interval':
                        continue
                    cfg_args[attr] = plugin['config'][attr]
                err, msg = comm.plugn_config(name=plugin['name'], **cfg_args)
                if err:
                    print(f"Error: {err} {msg} configuring {smplr_name}")
                    continue
    return True

def start_samplers(comms, samplers, daemons):
    for smplr_group in samplers:
        for smplr in daemons[smplr_group]:
            smplr_host = daemons[smplr_group][smplr]
            smplr_name = smplr_host['name']
            # Establish communicator to each ldmsd sampler
            comm = comms[samplers[smplr_group]['daemons']][smplr_host['name']]
            err, res = comm.daemon_status()
            if err:
                print(f"Lost connectivity to {smplr_name} err {err}")
                return False
            smplrs = comm.smplr_status()
            for plugin in samplers[smplr_group]['plugins']:
                print(f'Starting.. {plugin["name"]} on {smplr_name}')
                if 'interval' in plugin:
                    interval = cvt_intrvl_str_to_us(plugin['interval'])
                else:
                    interval = 1000000
                if 'offset' in plugin:
                    offset = cvt_intrvl_str_to_us(plugin['offset'])
                else:
                    offset = None
                rc, msg = comm.plugn_start(plugin['name'], interval, offset)
                if rc:
                    print(f'Error {rc} starting {plugin["name"]} on {smplr_name} \n'\
                          f'Error: {msg}')
                    continue
    return True

def stop_samplers(comms, samplers, daemons):
    for smplr_group in samplers:
        for smplr in daemons[smplr_group]:
            smplr_host = daemons[smplr_group][smplr]
            smplr_name = smplr_host['name']
            # Establish communicator to each ldmsd sampler
            comm = comms[samplers[smplr_group]['daemons']][smplr_host['name']]
            err, res = comm.daemon_status()
            if err:
                print(f"Lost connectivity to {smplr_name} err {err}")
                return False
            smplrs = comm.smplr_status()
            for plugn in smplrs[1]:
                rc, msg = comm.plugn_stop(plugn['name'])
                if rc:
                    print(f'Error {rc} stopping {plugn["name"]} on {smplr_name} '
                          f'Error: {msg}')
                    continue
                else:
                    print(f'Stopped {plugn["name"]} on {smplr_name}')
    return True

def restart_samplers(comms, samplers, daemons):
    # Stop all sampler plugins on samplers specified and restart them
    # Does not reconfigure samplers
    rc = stop_samplers(comms, samplers, daemons)
    rc = start_samplers(comms, samplers, daemons)

def add_producers(comms, producers, aggregators, endpoints, agg_state):
    """Add producers to Aggregators

    Producers are added to all Aggregators in the group. They
    are all in the STOPPED state initially. The start_producers
    function will start producers on the Aggregator to which
    they have been assigned by the load balancer.

    This should reduce the latency of rebalancing as the
    producer only needs to be started, not added, configured
    and started when it assumes control from a failed peer.
    """
    for grp_name in producers:
        grp_prods = set([ prod for prod in producers[grp_name]])
        prod_dict = producers[grp_name]
        # Query the producer list on each aggregator in the group
        for agg_host in agg_hosts(grp_name, aggregators, endpoints):
            agg_name = agg_host['name']
            if agg_state[agg_name] != 'ready':
                continue
            comm = comms[grp_name][agg_name]
            err, res = comm.prdcr_status()
            if err:
                print(f"Lost connectivity to {agg_name} err {err}")
                return False
            else:
                agg_config = set({ prod['name'] for prod in res })
            add_prods = grp_prods - agg_config
            print(f"Adding {len(add_prods)} producers to agg {agg_name}")
            auth = None
            if len(add_prods) > 0:
                ep = next(iter(add_prods))
                dmn_grp = prod_dict[ep]['daemons']
                auth = endpoints[dmn_grp][ep]['auth']['name']
                if 'conf' in endpoints[dmn_grp][ep]['auth']:
                    auth_opt = endpoints[dmn_grp][ep]['auth']['conf']
                else:
                    auth_opt = None

            if auth is not None:
                rc, msg = comm.auth_add(auth, auth_opt)
                if rc:
                    print(f'Error adding authentication domain {auth} to aggregator {agg_name}\n')
                    print(f'Error code {rc}: {msg}\n')
            for prod_name in add_prods:
                prod = prod_dict[prod_name]
                ep_name = prod['endpoint']
                endpoint = endpoints[dmn_grp][ep_name]
                comm.prdcr_add(
                            prod_name, prod['type'],
                            endpoint['xprt'], endpoint['addr'], endpoint['port'],
                            prod['reconnect'], auth=auth)
                if 'subscribe' in prod:
                    for sub in prod['subscribe']:
                        if 'regex' in sub:
                            regex = sub['regex']
                        else:
                            regex = '.*'
                        comm.prdcr_subscribe(stream=sub['stream'], regex=regex)
    return True

def add_updaters(comms, updaters, aggregators, daemons, agg_state):
    """
    Add updaters to Aggregators
    """
    for grp_name in updaters:
        grp_updaters = updaters[grp_name]
        for updtr_name in grp_updaters:
            updtr = grp_updaters[updtr_name]
            for agg_host in agg_hosts(grp_name, aggregators, daemons):
                if agg_state[agg_host['name']] != 'ready':
                    continue
                comm = comms[grp_name][agg_host['name']]
                # the updater may already exist, in which case the add will fail
                if 'offset' in updtr:
                    offset = updtr['offset']
                else:
                    offset = None
                if 'push' in updtr:
                    rc, msg = comm.updtr_add(updtr_name, interval=updtr['interval'], offset=offset, push=updtr['push'])
                elif 'auto' in updtr and updtr['auto'] == "True":
                    rc, msg = comm.updtr_add(updtr_name, interval=updtr['interval'], offset=offset, auto=updtr['auto'])
                else:
                    rc, msg = comm.updtr_add(updtr_name, interval=updtr['interval'], offset=offset)

                if rc:
                    print(f'Error {rc}: {msg} adding updater {updtr_name} to {agg_host["name"]}')

                for regex in updtr['producers']:
                    comm.updtr_prdcr_add(updtr_name, regex['regex'])
                for regex in updtr['sets']:
                    if 'field' in regex:
                        field = regex['field']
                    else:
                        field = None
                    err, msg = comm.updtr_match_add(updtr_name, regex['regex'], match=field)

                err, msg = comm.updtr_start(updtr_name)
                if err != 0 and err != errno.EBUSY:
                    print(f"Error {err} : {msg} starting {updtr_name}")
                    return False
    return True

def add_stores(comms, stores, aggregators, daemons, agg_state):
    """
    Add stores to Aggregators
    """
    for grp_name in stores:
        grp_stores = stores[grp_name]
        for store_name in grp_stores:
            store = grp_stores[store_name]
            for agg_host in agg_hosts(grp_name, aggregators, daemons):
                if agg_state[agg_host['name']] != 'ready':
                    continue
                comm = comms[grp_name][agg_host['name']]
                # Load and configure the plugin required by the store
                plugin = store['plugin']
                plugin_name = plugin['name']
                plugin_config = plugin['config']
                err, res = comm.plugn_load(plugin_name)
                err, res = comm.plugn_config(plugin_name, **plugin_config)
                if 'flush' in store:
                    flush = store['flush']
                else:
                    flush = None
                ## the stores may already exist, in which case the add will fail
                err, res = comm.strgp_add(store_name,
                                          plugin_name,
                                          store['container'],
                                          store['schema'],
                                          flush=flush)
                ## Add producers
                err, res = comm.strgp_prdcr_add(store_name, ".*")

                # Start the store
                err, msg = comm.strgp_start(store_name)
                if err != 0 and err != errno.EBUSY:
                    print(f"Error {err} : {msg} starting {store_name}")
                    return False
    return True

def start_producers(comms, groups):
    """
    Query each aggregator for its producer status. If it is already
    running leave it alone, if it is running and not in the new group
    configuration, stop it.
    """
    for grp_name in groups:
        group = groups[grp_name]
        for grp_agg in group:
            agg = grp_agg['aggregator']
            agg_prods = set(grp_agg['producers'])
            comm = comms[grp_name][grp_agg['aggregator']['endpoint']]

            # get the current state
            err, res = comm.prdcr_status()
            if err:
                print(f"Error {err} querying state from aggregator {agg['name']}")
                continue

            start_prods = list(prod['name'] for prod in res \
                if prod['state'] == 'STOPPED' and prod['name'] in agg_prods)
            stop_prods = list(prod['name'] for prod in res \
                if prod['state'] != 'STOPPED' and prod['name'] not in agg_prods)

            print(f"Starting agg {agg['name']} {len(start_prods)} producers")
            for prod_name in start_prods:
                comm.prdcr_start(prod_name, regex=False)
            print(f"Stopping agg {agg['name']} {len(stop_prods)} producers")
            for prod_name in stop_prods:
                comm.prdcr_stop(prod_name, regex=False)

def query_ldmsd_state(client, comms):
    ldmsd_state = {}
    for grp_name in comms:
        group = comms[grp_name]
        for name in group:
            comm = group[name]
            if comm.state != comm.CONNECTED:
                comm.close()
                comm.reconnect()
            err, msg = comm.daemon_status()
            if err == 0:
                res = json.loads(msg)
                state = res['state']
            else:
                state = 'stopped'
            ldmsd_state[name] = state
    return ldmsd_state

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="LDMS Monitoring Cluster Manager")
    parser.add_argument("--cluster", metavar="FILE", required=True,
                        help="The name of the etcd cluster configuration file")
    parser.add_argument("--dump", action="store_true",
                        help="Dump the load-balanced aggregation configuration files")
    parser.add_argument("--prefix", metavar="STRING", required=True,
                        help="The prefix for the dumped aggregator configurations",
                        default="unknown")
    parser.add_argument("--start-aggregators", action='store_true',
                        help="Start ldms aggregator daemon's")
    parser.add_argument("--version", metavar="VERSION",
                        help="The OVIS version for the output syntax (4 or 5), default is 4",
                        default=4)
    args = parser.parse_args()

    # Load the cluster configuration file. This configures the daemons
    # that support the key/value configuration database
    etcd_fp = open(args.cluster)
    etcd_spec = yaml.safe_load(etcd_fp)

    # All keys in the DB are prefixed with the cluster name, 'pfx'. So we can
    # have multiple monitoring hosted by the same consensus cluster.
    pfx = etcd_spec['cluster']
    etcd_hosts = ()
    for h in etcd_spec['members']:
        etcd_hosts += (( h['host'], h['port'] ),)

    maestro = MaestroMonitor(args.prefix, etcd_hosts, args)

    # At each load-balance group level the aggregators have all
    # producers; however, only the producers assigned to each
    # aggregator are started. This practice reduces the latency
    # when an aggregator needs to pick up the load from a
    # failed peer
    maestro.cluster_monitor()
    # evq = queue.Queue()
    # mthread = threading.Thread(target=cluster_monitor)
    # mthread.start()
    # evq.join()

    # config = Cluster(etcd_spec, conf_spec)
    # config.balance()
    # config.commit()
    # if args.dump:
    #    config.dump_config(args.prefix, args.version)
